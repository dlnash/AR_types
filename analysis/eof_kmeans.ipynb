{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis in T-mode\n",
    "* K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence\n",
    "from eofs import *\n",
    "from ar_funcs import ar_climatology\n",
    "from kmeans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "home = Path.home()                                # users home directory\n",
    "root = home/'DATA'/'repositories'/'AR_types'      # project root directory\n",
    "path_to_data = home/'DATA'/'data'                 # project data -- read only\n",
    "path_to_out  = root/'out'                         # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = root/'figs'                        # figures\n",
    "\n",
    "# check that path exists\n",
    "path_to_figs.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data into pandas DataFrame\n",
    "filename = path_to_data / 'CH1_generated_data/ar_catalog_fraction_HASIAsubregions.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "\n",
    "#For MERRA2 IVT\n",
    "ds = ds.sel(time=slice('1980-01-01', '2018-12-31'))\n",
    "\n",
    "R01_dates = ar_climatology(ds.R01, 0.3)\n",
    "R02_dates = ar_climatology(ds.R02, 0.3)\n",
    "R03_dates = ar_climatology(ds.R03, 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 0.3\n",
    "\n",
    "df = ds.to_dataframe()\n",
    "## drop lev and ens cols\n",
    "df = df.drop(columns=['lev', 'ens'])\n",
    "## resample to daily\n",
    "df = df.resample('1D').mean()\n",
    "# Add column of AR days based on threshold\n",
    "# (no LLJ day eq 0; LLJ day eq 1)\n",
    "df['ar'] = 0\n",
    "idx = (df['R01'] > thres) | (df['R02'] > thres) | (df['R03'] > thres)\n",
    "df.loc[idx, 'ar'] = 1\n",
    "\n",
    "# Add column of AR locations \n",
    "# ('R01', 'R02', 'R03', 'R01/R02', 'R02/R03', 'R01/R03', 'R01/R02/R03', nan)\n",
    "df['location'] = np.nan\n",
    "\n",
    "idx = (df['R01'] >= thres) & (df['R02'] < thres) & (df['R03'] < thres)\n",
    "df.loc[idx, 'location'] = 'R01'\n",
    "\n",
    "idx = (df['R01'] < thres) & (df['R02'] >= thres) & (df['R03'] < thres)\n",
    "df.loc[idx, 'location'] = 'R02'\n",
    "\n",
    "idx = (df['R01'] < thres) & (df['R02'] < thres) & (df['R03'] >= thres)\n",
    "df.loc[idx, 'location'] = 'R03'\n",
    "\n",
    "idx = (df['R01'] >= thres) & (df['R02'] >= thres) & (df['R03'] < thres)\n",
    "df.loc[idx, 'location'] = 'R01/R02'\n",
    "\n",
    "idx = (df['R01'] < thres) & (df['R02'] >= thres) & (df['R03'] >= thres)\n",
    "df.loc[idx, 'location'] = 'R02/R03'\n",
    "\n",
    "idx = (df['R01'] >= thres) & (df['R02'] < thres) & (df['R03'] >= thres)\n",
    "df.loc[idx, 'location'] = 'R01/R03'\n",
    "\n",
    "idx = (df['R01'] >= thres) & (df['R02'] >= thres) & (df['R03'] >= thres)\n",
    "df.loc[idx, 'location'] = 'R01/R02/R03'\n",
    "\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERRA2 reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lat/lon grid\n",
    "lonmin = 0\n",
    "lonmax = 120\n",
    "latmin = 0\n",
    "latmax =  50\n",
    "\n",
    "### MERRA2 DATA ###\n",
    "def preprocess(ds):\n",
    "    '''keep only selected lats and lons'''\n",
    "    return ds.sel(lat=slice(latmin, latmax), lon=slice(lonmin, lonmax))\n",
    "\n",
    "filepath_pattern = '/home/sbarc/students/nash/data/MERRA2/anomalies/IVT/daily_*.nc'\n",
    "merra = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "print('ds size in GB {:0.2f}\\n'.format(merra.nbytes / 1e9))\n",
    "\n",
    "# ## Use if IVT\n",
    "# new_times = pd.date_range('1980-01-01-09', '2018-12-31-09', freq='D')\n",
    "# ds_z['time'] = new_times\n",
    "# ds_z['ivt'] = np.sqrt(ds_z.ivtx**2 + ds_z.ivty**2)\n",
    "merra\n",
    "# ds_z = ds_z.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datafiles into xarray datasets\n",
    "# f2 = xr.open_dataset(path_to_data/'era5.sam.05dg.ivtn.1979-2016.nc')\n",
    "\n",
    "# Add LLJ time series to era5; set as coordinate variables\n",
    "merra['ar'] = ('time', df.ar)\n",
    "merra = merra.set_coords('ar')\n",
    "\n",
    "merra['location'] = ('time', df.location)\n",
    "merra = merra.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(merra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim date range\n",
    "start_date = '1980-12-01'\n",
    "end_date = '2018-02-28'\n",
    "idx = slice(start_date, end_date)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# Select NDJFM months\n",
    "idx = (merra.time.dt.month >= 12) | (merra.time.dt.month <= 2)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# Select AR days\n",
    "idx = (merra.ar >= 1)\n",
    "merra_ar = merra.sel(time=idx)\n",
    "\n",
    "# print results\n",
    "print(merra_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of independent AR events\n",
    "\n",
    "years = np.arange(1980, 2018) \n",
    "nyrs = len(years)\n",
    "total_events = 0\n",
    "for k in range(nyrs-1):    \n",
    "    # Extract single DJF season\n",
    "    date1 = \"{}-12-01\".format(years[k])\n",
    "    date2 = \"{}-02-28\".format(years[k+1])\n",
    "    x = merra.ar.sel(time=slice(date1,date2)).values\n",
    "    # Count AR events in that season\n",
    "    tags, tmp = persistence(x)\n",
    "    # Add to running event count\n",
    "    total_events += tmp\n",
    "\n",
    "print(\"Number of independent AR events: \", total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climatology and Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mean IVT of AR days in DJF\n",
    "# merra_ar_clim = merra_ar.mean(dim='time')\n",
    "# #print(merra_ar_clim, '\\n')\n",
    "\n",
    "# # IVT Anomalies\n",
    "# merra_ar['ivtx'] = era_llj.ivte - era_llj_clim.ivte\n",
    "# era_llj['ivtn_anom'] = era_llj.ivtn - era_llj_clim.ivtn\n",
    "# #print(era_llj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load merra_ar dataset into memory\n",
    "merra_ar = merra_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(in_array):\n",
    "    ''' Remove the mean of an array along the first dimension.\n",
    "    \n",
    "    If *True*, the mean along the first axis of *dataset* (the\n",
    "    time-mean) will be removed prior to analysis. If *False*,\n",
    "    the mean along the first axis will not be removed. Defaults\n",
    "    to *True* (mean is removed).\n",
    "    The covariance interpretation relies on the input data being\n",
    "    anomaly data with a time-mean of 0. Therefore this option\n",
    "    should usually be set to *True*. Setting this option to\n",
    "    *True* has the useful side effect of propagating missing\n",
    "    values along the time dimension, ensuring that a solution\n",
    "    can be found even if missing values occur in different\n",
    "    locations at different times.\n",
    "    '''\n",
    "    # Compute the mean along the first dimension.\n",
    "    mean = in_array.mean(axis=0, skipna=False)\n",
    "    # Return the input array with its mean along the first dimension\n",
    "    # removed.\n",
    "    return (in_array - mean)\n",
    "\n",
    "def remove_missing_values(X):\n",
    "    # Find the indices of values that are missing in the whole row\n",
    "    nonMissingIndex = np.where(np.logical_not(np.isnan(X[0])))[0]\n",
    "    # Remove missing values from the design matrix.\n",
    "    dataNoMissing = X[:, nonMissingIndex]\n",
    "    return nonMissingIndex, dataNoMissing\n",
    "\n",
    "def valid_nan(in_array):\n",
    "    inan = np.isnan(in_array)\n",
    "    return (inan.any(axis=0) == inan.all(axis=0)).all()\n",
    "\n",
    "def standardize_and_flatten_arrays(arr1, arr2):\n",
    "\n",
    "    # Data dimensions with missing values removed\n",
    "    ntim, npts = arr1.shape\n",
    "\n",
    "    # Transpose arrays to get [space x time]\n",
    "    X1 = arr1.T\n",
    "    X2 = arr2.T\n",
    "\n",
    "    # Standardize by columns\n",
    "    x1std = np.std(X1, axis=0)\n",
    "    X1s = X1 / x1std\n",
    "\n",
    "    x2std = np.std(X2, axis=0)\n",
    "    X2s = X2 / x2std\n",
    "\n",
    "    # Combine variables into single data matrix Xs\n",
    "    Xs = np.empty((nvar*npts,ntim))\n",
    "    Xs[0:npts,:] = X1s\n",
    "    Xs[npts:,:]  = X2s\n",
    "    print(Xs.shape)\n",
    "\n",
    "    # Check that column means=0 and std dev=1\n",
    "    test = np.mean(np.mean(Xs, axis=0))\n",
    "    print(\"Column means: \", np.round(test,2))\n",
    "    test = np.mean(np.std(Xs, axis=0))\n",
    "    print(\"Column std: \", np.round(test,2))\n",
    "    \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the variables by removing long-term mean\n",
    "var1 = center_data(merra_ar.ivtx)\n",
    "var2 = center_data(merra_ar.ivty)\n",
    "\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "wgts = spatial_weights(merra_ar.lat)\n",
    "var1 = var1*wgts\n",
    "var2 = var2*wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract variables as numpy arrays\n",
    "var1 = var1.values\n",
    "var2 = var2.values\n",
    "\n",
    "# Data dimensions\n",
    "ntim, nlat, nlon = var1.shape\n",
    "npts = nlat*nlon\n",
    "nvar = 2\n",
    "\n",
    "# Reshape into 2D arrays by flattening the spatial dimension\n",
    "tmp1 = np.reshape(var1, (ntim, npts))\n",
    "tmp2 = np.reshape(var2, (ntim, npts))\n",
    "\n",
    "# Remove missing data\n",
    "tmp1_idx, tmp1_miss = remove_missing_values(tmp1)\n",
    "tmp2_idx, tmp2_miss = remove_missing_values(tmp2)\n",
    "\n",
    "## Test if the removal of nans was successful\n",
    "print(valid_nan(tmp1_miss), valid_nan(tmp2_miss))\n",
    "\n",
    "## Standardize and flatten variable arrays\n",
    "Xs = standardize_and_flatten_arrays(tmp1_miss, tmp2_miss)\n",
    "Xs_nomiss = standardize_and_flatten_arrays(tmp1, tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute eigenvalues & eigenvectors\n",
    "evals, evecs = calc_eofs(Xs)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_variance(eig):\n",
    "    var_eig = eig/sum(eig)*100\n",
    "    return var_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2: Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot data\n",
    "xvals = np.arange(neofs) + 1\n",
    "ax.bar(xvals, pctvar[0:neofs], yerr=err[0:neofs], \n",
    "       color='tab:blue', alpha=0.8)\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel('EOF')\n",
    "ax.set_xticks(xvals)\n",
    "\n",
    "# y-axis\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "yticks = np.arange(0,16,3)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticks) \n",
    "\n",
    "# save fig\n",
    "filepath = path_to_figs / 'fig2.png'\n",
    "plt.savefig(filepath, dpi=300)\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neofs = 19\n",
    "loads = loadings(evals, evecs, neofs)\n",
    "\n",
    "print(loads.shape)\n",
    "print(np.round(loads,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eigenvalues, eigenvectors, and loadings\n",
    "\n",
    "neofs = 4   # number of EOFs to save (evecs, loadings3)\n",
    "\n",
    "outfile = path_to_out / 'eigenvalues.txt'\n",
    "np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "outfile = path_to_out / 'eigenvectors.txt'\n",
    "np.savetxt(outfile, evecs[:,0:neofs], fmt='%.5f', delimiter=',')\n",
    "\n",
    "outfile = path_to_out / 'loadings.txt'\n",
    "np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate principal components (spatial modes)\n",
    "neofs = 19\n",
    "pcs = calc_pcs(Xs_nomiss, evecs, neofs)\n",
    "# pcs = calc_pcs(Xs, evecs, neofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npts = len(tmp1_idx)\n",
    "\n",
    "# # Combine missing idx into single data matrix\n",
    "# tmp_idx = np.empty((nvar*npts))\n",
    "# print(tmp_idx.shape)\n",
    "# tmp_idx[0:npts] = tmp1_idx\n",
    "# tmp_idx[npts:]  = tmp2_idx\n",
    "# print(tmp_idx.shape)\n",
    "\n",
    "# ## create an array of nans that is the size of the original unmasked array\n",
    "# pcs_comb = np.ones([neofs, nvar*nlat*nlon], dtype=Xs.dtype) * np.NaN\n",
    "# ## fill in array with values of pcs at indexed values from tmp_idx\n",
    "# for count, ele in enumerate(tmp_idx):\n",
    "#     pcs_comb[:, int(ele)] = pcs[:,count]\n",
    "# print(pcs_comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pcs into separate arrays for each variable\n",
    "tmp1 = pcs[:,0:npts]\n",
    "tmp2 = pcs[:,npts:]\n",
    "# tmp1 = pcs_comb[:,0:nlat*nlon]\n",
    "# tmp2 = pcs_comb[:,nlat*nlon:]\n",
    "\n",
    "# Reshape spatial dim back to 2D map\n",
    "pcmodes_var1 = np.reshape(tmp1, (neofs,nlat,nlon))\n",
    "pcmodes_var2 = np.reshape(tmp2, (neofs,nlat,nlon))\n",
    "#print(pcmodes_var1.shape, pcmodes_var2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3: Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Plot of Spatial Modes\n",
    "\n",
    "# number of eofs to plot\n",
    "neofs = 4\n",
    "\n",
    "# Data for plotting\n",
    "lons = merra_ar.lon.data\n",
    "lats = merra_ar.lat.data\n",
    "udat = pcmodes_var1[0:neofs,:,:]\n",
    "vdat = pcmodes_var2[0:neofs,:,:]\n",
    "# data = np.sqrt(udat**2 + vdat**2)\n",
    "data = udat**2\n",
    "print(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, 111, axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), axes_pad = 0.55,\n",
    "                cbar_location='bottom', cbar_mode='single',\n",
    "                cbar_pad=0.0, cbar_size='2.5%',label_mode='')\n",
    "\n",
    "#newcmap = cmocean.tools.crop_by_percent(cmo.matter, 15, which='max', N=None)\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Add contour fill plot\n",
    "    clevs = np.arange(0,100,5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"Blues\", extend='max')\n",
    "    # add vectors\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=20)      \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('kg m$^{-1}$ s$^{-1}$', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "filepath = path_to_figs / 'eofs.png'\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e09951285152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mneofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mxdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mneofs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Elbow plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loads' is not defined"
     ]
    }
   ],
   "source": [
    "# Determine optimal K\n",
    "\n",
    "# maximum number of clusters (number of iterations)\n",
    "kmax =15\n",
    "# number of eofs\n",
    "neofs = 4\n",
    "# input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Elbow plot\n",
    "outfile = path_to_figs / 'xfig1.png'\n",
    "plot_optimal_k(xdata, kmax, filename=outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means cluster analysis\n",
    "\n",
    "# Number of clusters\n",
    "nk = 4\n",
    "\n",
    "# Input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Compute k means and assign each point to a cluster\n",
    "kmeans = KMeans(n_clusters=nk)\n",
    "kmeans.fit(xdata)\n",
    "cluster = kmeans.predict(xdata)\n",
    "\n",
    "# LLJ category labels (llj days only)\n",
    "llj_cat = cluster + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of days in each cluster\n",
    "klabels, counts = np.unique(llj_cat, return_counts=True)\n",
    "\n",
    "# Save counts to txt file\n",
    "res = np.column_stack((klabels,counts))\n",
    "headstr = 'LLJ_TYPE, COUNT'\n",
    "outfile = path_to_out / 'k_counts.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%d', header=headstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids (nclust x neofs)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Save centroids to txt file\n",
    "res = np.column_stack((klabels,centroids))\n",
    "headstr = \"LLJ_TYPE, EOF1, EOF2, EOF3, EOF4\"\n",
    "outfile = path_to_out / 'centroids.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%s', header=headstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save LLJ category labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save LLJ location, loadings (EOF1-4), and category label (LLJ days only)\n",
    "\n",
    "# Vector of LLJ dates\n",
    "dates_lljDays = era_llj.time.values\n",
    "\n",
    "# Create new dataframe\n",
    "data = {'LOC':era_llj.location.values,\n",
    "        'EOF1':loads[:,0],\n",
    "        'EOF2':loads[:,1],\n",
    "        'EOF3':loads[:,2],\n",
    "        'EOF4':loads[:,3],\n",
    "        'LLJ_CAT':llj_cat}\n",
    "df_out = pd.DataFrame(data, index=dates_lljDays)\n",
    "print(df_out)\n",
    "\n",
    "# Export dataframe as csv\n",
    "outfile = path_to_out / 'sallj-types-loadings.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save time series of all NDJFM days with SALLJ types\n",
    "\n",
    "# Arrays with ALL NDJFM days\n",
    "dates_allDays = era.time.values\n",
    "llj_cat_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "\n",
    "# Loop over llj days and match to llj_full \n",
    "for i, date in enumerate(dates_lljDays):\n",
    "    idx = np.where(dates_allDays == date)\n",
    "    llj_cat_allDays[idx] = llj_cat[i]  \n",
    "\n",
    "# Create dataframe\n",
    "data = {'LLJ_CAT':llj_cat_allDays}\n",
    "df_out = pd.DataFrame(data, index=dates_allDays)\n",
    "print(df_out)\n",
    "\n",
    "outfile = path_to_out / 'sallj-types-ndjfm.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ar_types)",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
