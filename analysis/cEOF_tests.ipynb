{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis - testing different configurations to see which cEOF captures the most varaince of precipitation related to AR events\n",
    "\n",
    "  1. cEOF 250 hPa H, U, V in extratropics, 850 hPa Q, U, V in tropics\n",
    "  2. cEOF 500 hPa H, U, V in extratropics, 500 hPa Q, U, V in tropics\n",
    "  3. cEOF 500 hPa H, U, V in extratropics, 500 hPa H, U, V in tropics\n",
    "  4. cEOF 500 hPa H, U, V in full domain [0-120 E, 0-50N]\n",
    "  5. cEOF IVTu and IVTv in full domain\n",
    "  6. cEOF 500 hPa H, U, V and 850 hPa Q in full domain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import cycle\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mticker\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from scipy import stats\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence, select_months\n",
    "from eofs import *\n",
    "from ar_funcs import preprocess_ar_area_subregions\n",
    "from kmeans import *\n",
    "import nclcmaps as ncl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/nash/DATA/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/test/k3/'      # figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R01</th>\n",
       "      <th>R02</th>\n",
       "      <th>R03</th>\n",
       "      <th>ar</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979-01-01</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-01-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-01-03</th>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-01-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-01-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 R01  R02  R03  ar location\n",
       "time                                       \n",
       "1979-01-01  0.000000  0.0  0.0   0      NaN\n",
       "1979-01-02  0.000000  0.0  0.0   0      NaN\n",
       "1979-01-03  0.005102  0.0  0.0   0      NaN\n",
       "1979-01-04  0.000000  0.0  0.0   0      NaN\n",
       "1979-01-05  0.000000  0.0  0.0   0      NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select MERRA2 or ERA5\n",
    "reanalysis = 'era5'\n",
    "\n",
    "if reanalysis == 'era5':\n",
    "    start_date = '1979-01-01'\n",
    "    end_date = '2018-12-31'\n",
    "    filename = 'ar_catalog_ERAI_fraction_HASIAsubregions.nc'\n",
    "## if MERRA2\n",
    "else:\n",
    "    start_date = '1980-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    filename = 'ar_catalog_fraction_HASIAsubregions.nc'\n",
    "    \n",
    "f1 = path_to_data + 'CH1_generated_data/' + filename\n",
    "ds = xr.open_dataset(f1)\n",
    "# Set dates\n",
    "ds = ds.sel(time=slice(start_date, end_date))\n",
    "## Preprocess AR subregions - get dataframe of AR days based on area threshold\n",
    "df = preprocess_ar_area_subregions(df=ds.to_dataframe(), thres=0.3)\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configuration file for season dictionary choice\n",
    "yaml_doc = '../data/config.yml'\n",
    "config1 = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "ssn = 'mam'\n",
    "ar_dict = config1[ssn]\n",
    "\n",
    "# import configuration file for ceof dictionary choice\n",
    "## test number - NOTE as of 7/1/2020 sticking with test 4\n",
    "yaml_doc = '../data/ceof_config.yml'\n",
    "config2 = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "config_dict = config2['test4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set variable names (for saving data/figs)\n",
    "testname = config_dict['name']\n",
    "eofmode = 't' # s or t\n",
    "dispmat = 'cor' # dispersion matrix type correlation/covariance\n",
    "pathvar = config_dict['pathvar']\n",
    "\n",
    "# Select lat/lon grid \n",
    "# Tropics/Extratropics Domain\n",
    "lonmin = config_dict['latlon'][0]\n",
    "lonmax = config_dict['latlon'][1]\n",
    "latmin = config_dict['latlon'][2]\n",
    "latmax = config_dict['latlon'][3]\n",
    "\n",
    "lev = config_dict['levs']\n",
    "\n",
    "# for figure names for testing different configurations\n",
    "fname_id = config_dict['fnameID'] + eofmode + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + dispmat\n",
    "print(fname_id)\n",
    "\n",
    "if pathvar == 'huvq':\n",
    "    def preprocess(ds):\n",
    "        '''keep only selected lats and lons'''\n",
    "        return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), level=lev)\n",
    "if pathvar == 'ivt':\n",
    "        def preprocess(ds):\n",
    "            '''keep only selected lats and lons'''\n",
    "            return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax))\n",
    "\n",
    "# open anomaly data\n",
    "filepath_pattern = path_to_data + 'ERA5/{0}/anomalies/daily_filtered_anomalies_{0}_*.nc'.format(pathvar)\n",
    "    \n",
    "    \n",
    "f2 = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "\n",
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pathvar == 'huvq':\n",
    "\n",
    "    # Create new dataset to rename lat lon\n",
    "    ds = xr.Dataset({'H': (['time', 'level', 'lat', 'lon'], f2['z'].values),\n",
    "                     'U': (['time', 'level',  'lat', 'lon'], f2['u'].values),\n",
    "                     'V': (['time', 'level',  'lat', 'lon'], f2['v'].values),\n",
    "                     'QV': (['time', 'level',  'lat', 'lon'], f2['q'].values)},\n",
    "                          coords={'time': (['time'], f2.time.values),\n",
    "                                  'level': (['level'], f2.level.values),\n",
    "                                  'lat': (['lat'], f2.latitude.values),\n",
    "                                  'lon': (['lon'], f2.longitude.values)})\n",
    "\n",
    "if pathvar == 'ivt':\n",
    "    ds = xr.Dataset({'ivte': (['time', 'lat', 'lon'], f2['p71.162'].values),\n",
    "                     'ivtn': (['time', 'lat', 'lon'], f2['p72.162'].values)},\n",
    "                          coords={'time': (['time'], f2.time.values),\n",
    "                                  'lat': (['lat'], f2.latitude.values),\n",
    "                                  'lon': (['lon'], f2.longitude.values)})\n",
    "\n",
    "\n",
    "ds\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AR time series to merra; set as coordinate variables\n",
    "ds['ar'] = ('time', df.ar)\n",
    "ds = ds.set_coords('ar')\n",
    "\n",
    "ds['location'] = ('time', df.location)\n",
    "ds = ds.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = slice(ar_dict['start_date'], ar_dict['end_date'])\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# Select months\n",
    "if ar_dict['mon_s'] > ar_dict['mon_e']:\n",
    "    idx = (ds.time.dt.month >= ar_dict['mon_s']) | (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "else:\n",
    "    idx = (ds.time.dt.month >= ar_dict['mon_s']) & (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# # Select AR days JUST IN R01\n",
    "# idx = (ds.ar >= 1) & (ds.location == 'R01')\n",
    "# Select AR days in all subregions\n",
    "idx = (ds.ar >= 1)\n",
    "ds_ar = ds.sel(time=idx)\n",
    "# ds_ar = ds\n",
    "\n",
    "# print results\n",
    "print(ds_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of independent AR events and their duration in days\n",
    "x = ds.ar\n",
    "event_id, total_events, duration = persistence(x)\n",
    "print('Total number of AR days in season: ', x.sum())\n",
    "print('Total number of independent AR events: ', total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load merra_ar dataset into memory\n",
    "ds_ar = ds_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of variable arrays\n",
    "    \n",
    "var_list = []\n",
    "for i in range(len(config_dict['varlist'])):\n",
    "    var1 = config_dict['varlist'][i]\n",
    "    domain = config_dict['domain'][i]\n",
    "    domain_bnds = config2['eof_domain'][domain]\n",
    "    lonslice = slice(domain_bnds[0], domain_bnds[1])\n",
    "    latslice = slice(domain_bnds[3], domain_bnds[2])\n",
    "    if pathvar == 'huvq':\n",
    "        lev = config_dict['varlev'][i]\n",
    "        var_list.append(ds_ar[var1].sel(lon=lonslice, lat=latslice, level=lev))\n",
    "    if pathvar == 'ivt':\n",
    "        var_list.append(ds_ar[var1].sel(lon=lonslice, lat=latslice))\n",
    "\n",
    "# Check that sizes of arrays match\n",
    "for i, in_array in enumerate(var_list):\n",
    "    # Extract variable as numpy array\n",
    "    var1 = in_array.values\n",
    "    print(var1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "var_list = spatial_weights(var_list)\n",
    "\n",
    "## Flatten data to [time x space]\n",
    "var_list = flatten_array(var_list)\n",
    "\n",
    "## Center data\n",
    "var_list = center_data(var_list)\n",
    "\n",
    "## Standardize Arrays by removing the mean and dividing by the standard deviation of the columns\n",
    "## For multivariate, place into single flattened array\n",
    "Xs = standardize_arrays(var_list, mode=eofmode, dispersion_matrix=dispmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute eigenvalues & eigenvectors\n",
    "R, evals, evecs = calc_eigs(z=Xs, mode=eofmode)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot data\n",
    "xvals = np.arange(neofs) + 1\n",
    "ax.bar(xvals, pctvar[0:neofs], yerr=err[0:neofs], \n",
    "       color='tab:blue', alpha=0.8)\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel('EOF')\n",
    "ax.set_xticks(xvals)\n",
    "\n",
    "# y-axis\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "yticks = np.arange(0,16,3)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticks) \n",
    "\n",
    "# save fig\n",
    "filepath = path_to_figs  + testname + ssn + '_exp_variance' + '.png'\n",
    "print(filepath)\n",
    "plt.savefig(filepath, dpi=300)\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOFs and PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose neofs and npcs based on North Test\n",
    "# to save, plot, etc.\n",
    "neofs = 3\n",
    "npcs = neofs\n",
    "\n",
    "# Calculate EOFS (spatial modes)\n",
    "eofs = calc_eofs(Xs, evecs, evals, neofs, mode=eofmode)\n",
    "\n",
    "# Split eofs into separate arrays for each variable\n",
    "ntim, nlat, nlon = var1.shape\n",
    "npts = nlat*nlon\n",
    "nvar = len(var_list)\n",
    "# Reshape spatial dim back to 2D map\n",
    "eofmodes = var_list\n",
    "for i in np.arange(len(var_list)):\n",
    "    tmp = eofs[:,i*npts:(i+1)*npts]\n",
    "    eofmodes[i] = np.reshape(tmp, (neofs,nlat,nlon))\n",
    "    \n",
    "# Calculate PCs (time coefficients)\n",
    "pcs = calc_pcs(Xs, evecs, evals, npcs, mode=eofmode)\n",
    "# results in [ntim, npcs] to plot in PC plot\n",
    "\n",
    "## loadings*\n",
    "## in the case of t-mode these are our \"pcs\" or time-coefficients\n",
    "## in the case of s-mode, these are our \"eofs\" or spatial loadings\n",
    "loads = pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Eigenvectors, eigenvalues, and temporal loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save eigenvalues, eigenvectors, and loadings\n",
    "\n",
    "\n",
    "# outfile = path_to_out + 'eigenvalues_'+ fname_id + '.txt'\n",
    "# np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "# outfile = path_to_out + 'eigenvectors_'+ fname_id + '.txt'\n",
    "# np.savetxt(outfile, evecs[:,0:neofs], fmt='%.5f', delimiter=',')\n",
    "\n",
    "# outfile = path_to_out + 'loadings_'+ fname_id + '.txt'\n",
    "# np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting \n",
    "lons = ds_ar.lon.data\n",
    "lats = ds_ar.lat.data\n",
    "udat = eofmodes[1][0:neofs,:,:]\n",
    "vdat = eofmodes[2][0:neofs,:,:]\n",
    "data = eofmodes[0][0:neofs,:,:]\n",
    "\n",
    "print(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "pc_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    pc_label.append(\"PC{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = path_to_figs + testname + ssn + '_spatial' + '.png'\n",
    "nrows = neofs\n",
    "ncols = 1\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='5%',\n",
    "                label_mode='')\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Contour Filled\n",
    "    clevs = np.arange(-30,31,5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    # Wind barbs / vectors\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=20) \n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('m', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal K\n",
    "\n",
    "# maximum number of clusters (number of iterations)\n",
    "kmax =15\n",
    "# input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Elbow plot\n",
    "outfile = path_to_figs + testname + ssn + '_elbow'\n",
    "plot_optimal_k(xdata, kmax, filename=outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal k by examining the kde of the eofs\n",
    "# make a dict of the first n eofs with column labels for df\n",
    "values = []\n",
    "for i in range(neofs):\n",
    "    values.append(loads[:,i])\n",
    "    \n",
    "keys = eof_label\n",
    "dicts = dict(zip(keys, values))\n",
    "# print(dicts)\n",
    "\n",
    "# Create new dataframe\n",
    "dates_allDays = ds_ar.time.values\n",
    "dates_arDays = ds_ar.time.values\n",
    "df_out = pd.DataFrame(dicts, index=dates_arDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot(df_test.EOF1, df_test.EOF2)\n",
    "\n",
    "g = sns.PairGrid(df_out, diag_sharey=True, corner=True)\n",
    "# g.map_upper(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw = 3)\n",
    "g = g.add_legend(fontsize=14)\n",
    "\n",
    "filepath = path_to_figs + fname_id + ssn + 'neof_' + str(neofs) + '_hist_kde'+ '.png'\n",
    "g.savefig(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means cluster analysis\n",
    "\n",
    "# Number of clusters\n",
    "nk = 4\n",
    "\n",
    "# Input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Compute k means and assign each point to a cluster\n",
    "kmeans = KMeans(n_clusters=nk)\n",
    "kmeans.fit(xdata)\n",
    "cluster = kmeans.predict(xdata)\n",
    "\n",
    "# LLJ category labels (llj days only)\n",
    "ar_cat = cluster + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of days in each cluster\n",
    "klabels, counts = np.unique(ar_cat, return_counts=True)\n",
    "\n",
    "# Save counts to txt file\n",
    "res = np.column_stack((klabels,counts))\n",
    "headstr = 'AR_TYPE, COUNT'\n",
    "outfile = path_to_out + fname_id + ssn + 'k_counts.txt'\n",
    "print(outfile)\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%d', header=headstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids (nclust x neofs)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Save centroids to txt file\n",
    "res = np.column_stack((klabels,centroids))\n",
    "keys[:0] = ['AR_TYPE']\n",
    "headstr = ', '.join(keys)\n",
    "# headstr = \"AR_TYPE, EOF1, EOF2, EOF3, EOF4\"\n",
    "outfile = path_to_out + fname_id + ssn + 'centroids.txt'\n",
    "print(outfile)\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%s', header=headstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save AR location, loadings (EOF1-n), and category label (AR days only)\n",
    "df_out['LOC'] = ds_ar.location.values\n",
    "df_out['AR_CAT'] = ar_cat\n",
    "\n",
    "# Export dataframe as csv\n",
    "outfile = path_to_out + fname_id + ssn + 'neof_' + str(neofs) + '_nk' + str(nk) + '_hma_AR-types-loadings.csv'\n",
    "df_out.to_csv(outfile)\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df_out,  hue=\"AR_CAT\", diag_sharey=False)\n",
    "g.map_upper(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw = 3, legend = True)\n",
    "g = g.add_legend(fontsize=14)\n",
    "\n",
    "filepath = path_to_figs + fname_id + ssn + 'neof_' + str(neofs) + '_hist_kde_arcat'+ '.png'\n",
    "g.savefig(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save time series of all DJF days with AR types\n",
    "\n",
    "# Arrays with ALL DJF days\n",
    "dates_allDays = ds.time.values\n",
    "ar_cat_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "\n",
    "# Loop over ar days and match to ar_full \n",
    "for i, date in enumerate(dates_arDays):\n",
    "    idx = np.where(dates_allDays == date)\n",
    "    ar_cat_allDays[idx] = ar_cat[i]\n",
    "\n",
    "# Create dataframe\n",
    "data = {'AR_CAT':ar_cat_allDays}\n",
    "df_out = pd.DataFrame(data, index=dates_allDays)\n",
    "print(df_out)\n",
    "\n",
    "outfile = path_to_out + fname_id + 'hma_AR-types-' + ssn + 'neof_' + str(neofs) + '_nk' + str(nk) + '.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Histogram of Difference in days between Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the AR Types into multiple columns\n",
    "# create a new df that breaks up the AR_CAT col so each category has its own column\n",
    "keys = []\n",
    "for k in range(nk):\n",
    "    keys.append(\"AR_CAT{:1d}\".format(k+1,))\n",
    "\n",
    "values = np.zeros((len(dates_arDays)))\n",
    "dicts = dict(zip(keys, values))\n",
    "\n",
    "df_cat = pd.DataFrame(dicts, index=dates_arDays)\n",
    "\n",
    "for k in range(nk):\n",
    "    idx = (df_out['AR_CAT'] == k+1)\n",
    "    col = \"AR_CAT{:1d}\".format(k+1,)\n",
    "    df_cat.loc[idx, col] = 1\n",
    "\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest(items, pivot):\n",
    "    '''Find the nearest date in a list compared to a single date'''\n",
    "    nearest=min(items, key=lambda x: abs(x - pivot))\n",
    "    timedelta = nearest-pivot\n",
    "    return timedelta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dates that match each AR type conditions\n",
    "x = []\n",
    "ns = []\n",
    "for k in range(nk):\n",
    "    col = \"AR_CAT{:1d}\".format(k+1,)\n",
    "    idx = (df_cat[col] > 0)\n",
    "    mask = df_cat.index[idx]\n",
    "    x.append(mask)\n",
    "    ns.append(len(mask))\n",
    "    \n",
    "print(len(x[0]), len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of days between Type 1 and the other types\n",
    "near = []\n",
    "data = []\n",
    "for i in range(nk-1):\n",
    "    for j in range(len(x[0])):\n",
    "        t1 = x[0][j]\n",
    "        t2 = pd.to_datetime(x[i+1])\n",
    "        near.append(nearest(items=t2, pivot=t1))\n",
    "    s = pd.Series(np.asarray(near))\n",
    "    data.append(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of dates that match AR1 conditions\n",
    "# idx = (df_cat.AR_CAT1 > 0)\n",
    "# x = df_cat.index[idx]\n",
    "\n",
    "# # list of dates that match AR2 conditions\n",
    "# idx = (df_cat.AR_CAT2 > 0)\n",
    "# y = df_cat.index[idx]\n",
    "\n",
    "# # # list of dates that match AR3 conditions\n",
    "# # idx = (df_cat.AR_CAT3 > 0)\n",
    "# # y2 = df_cat.index[idx]\n",
    "\n",
    "# ns = [len(x), len(y), len(y2)]\n",
    "# timedel = []\n",
    "# timedel2 = []\n",
    "# timedel3 = []\n",
    "# for i in range(len(x)):\n",
    "# #     t1 = pd.to_datetime(x.iloc[i])\n",
    "#     t1 = x[i]\n",
    "#     t2 = pd.to_datetime(y)\n",
    "# #     t3 = pd.to_datetime(y2)\n",
    "#     timedel.append(nearest(items=t2, pivot=t1))\n",
    "# #     timedel2.append(nearest(items=t3, pivot=t1))\n",
    "# # for i in range(len(y)):\n",
    "# #     timedel3.append(nearest(items=t3, pivot=y[i]))\n",
    "\n",
    "# # plot histogram\n",
    "# s = pd.Series(np.asarray(timedel))\n",
    "# # s2 = pd.Series(np.asarray(timedel2))\n",
    "# # s3 = pd.Series(np.asarray(timedel3))\n",
    "\n",
    "# # # truncate to a reasonable range\n",
    "# # s = s[(s > -50) & (s < 50)]\n",
    "# # s2 = s2[(s2 > -50) & (s2 < 50)]\n",
    "# # s3 = s3[(s3 > -50) & (s3 < 50)]\n",
    "\n",
    "# # data = [s, s2, s3]\n",
    "# data = [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot labels with each number of AR days in each cluster\n",
    "plt_labels = []\n",
    "for k in range(nk-1):\n",
    "    plt_labels.append(\"Type {0} (n={1}) and Type {2} (n={3})\".format(k+1, ns[k], k+2, ns[k+1]))\n",
    "    \n",
    "print(plt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = nk-1\n",
    "nplots = nrows*ncols\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(16,3))\n",
    "for i in np.arange(nplots):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    n, bins, patches = plt.hist(x=data[i], bins=np.arange(-48, 50, 2), color='#0504aa',\n",
    "                                alpha=0.7, rwidth=0.85, weights=np.ones(len(data[i])) / ns[i])\n",
    "#     print(len(data[i]))\n",
    "#     print(ns[i])\n",
    "    plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=None))\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Difference (days)')\n",
    "    plt.xticks(np.arange(-48, 50, 8)) \n",
    "    plt.ylabel('Frequency (% events)')\n",
    "    plt.title(plt_labels[i])\n",
    "    plt.ylim([0, .20])\n",
    "\n",
    "# Save figure\n",
    "filepath = path_to_figs + testname + ssn + '_histogram_perc'+ 'neof_' + str(neofs) + '_nk' + str(nk) + '.png'\n",
    "plt.savefig(filepath, dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ar_types)",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
