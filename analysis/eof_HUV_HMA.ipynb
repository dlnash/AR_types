{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis of H, U and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mticker\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence\n",
    "from eofs import *\n",
    "from ar_funcs import preprocess_ar_area_subregions\n",
    "from kmeans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/nash/DATA/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/cEOF_hma/'      # figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select MERRA2 or ERA5\n",
    "reanalysis = 'era5'\n",
    "if reanalysis == 'era5':\n",
    "    start_date = '1979-01-01'\n",
    "    end_date = '2018-12-31'\n",
    "    filename = 'ar_catalog_ERAI_fraction_HASIAsubregions.nc'\n",
    "## if MERRA2\n",
    "else:\n",
    "    start_date = '1980-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    filename = 'ar_catalog_fraction_HASIAsubregions.nc'\n",
    "    \n",
    "f1 = path_to_data + 'CH1_generated_data/' + filename\n",
    "ds = xr.open_dataset(f1)\n",
    "# Set dates\n",
    "ds = ds.sel(time=slice(start_date, end_date))\n",
    "## Preprocess AR subregions - get dataframe of AR days based on area threshold\n",
    "df = preprocess_ar_area_subregions(df=ds.to_dataframe(), thres=0.3)\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERRA2 reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds size in GB 2.16\n",
      "\n",
      "ds size in GB 4.33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Set variable names (for saving data/figs)\n",
    "var_names = 'HUV500'\n",
    "eofmode = 't' # s or t\n",
    "dispmat = 'cor' # dispersion matrix type correlation/covariance\n",
    "\n",
    "# Select lat/lon grid \n",
    "# HASIA Domain\n",
    "lonmin = 0\n",
    "lonmax = 120\n",
    "latmin = 0\n",
    "latmax =  50\n",
    "\n",
    "\n",
    "# for figure names for testing different configurations\n",
    "fname_id = var_names + eofmode + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + dispmat\n",
    "\n",
    "### MERRA2 DATA ###\n",
    "def preprocess(ds):\n",
    "    '''keep only selected lats and lons'''\n",
    "    return ds.sel(lat=slice(latmin, latmax), lon=slice(lonmin, lonmax))\n",
    "\n",
    "# open H data\n",
    "filepath_pattern = path_to_data + 'MERRA2/anomalies/H500/daily_*.nc'\n",
    "\n",
    "ds_h = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, concat_dim='time', combine='by_coords')\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds_h.nbytes / 1e9))\n",
    "\n",
    "## open UV data\n",
    "filepath_pattern = path_to_data + 'MERRA2/anomalies/UV500/daily_*.nc'\n",
    "ds_uv = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds_uv.nbytes / 1e9))\n",
    "\n",
    "## combine H and UV data into 1 ds object\n",
    "merra = xr.merge([ds_h, ds_uv.U, ds_uv.V])\n",
    "# merra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 101, lon: 193, time: 13880)\n",
      "Coordinates:\n",
      "    lev        float64 500.0\n",
      "  * lon        (lon) float64 0.0 0.625 1.25 1.875 ... 118.1 118.8 119.4 120.0\n",
      "  * lat        (lat) float64 0.0 0.5 1.0 1.5 2.0 ... 48.0 48.5 49.0 49.5 50.0\n",
      "  * time       (time) datetime64[ns] 1980-01-01T09:00:00 ... 2017-12-31T09:00:00\n",
      "    dayofyear  (time) int64 dask.array<chunksize=(366,), meta=np.ndarray>\n",
      "    ar         (time) int64 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "    location   (time) object nan nan nan nan nan nan ... nan nan nan nan nan nan\n",
      "Data variables:\n",
      "    H          (time, lat, lon) float64 dask.array<chunksize=(366, 101, 193), meta=np.ndarray>\n",
      "    U          (time, lat, lon) float64 dask.array<chunksize=(366, 101, 193), meta=np.ndarray>\n",
      "    V          (time, lat, lon) float64 dask.array<chunksize=(366, 101, 193), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Add AR time series to merra; set as coordinate variables\n",
    "merra['ar'] = ('time', df.ar)\n",
    "merra = merra.set_coords('ar')\n",
    "\n",
    "merra['location'] = ('time', df.location)\n",
    "merra = merra.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(merra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 101, lon: 193, time: 374)\n",
      "Coordinates:\n",
      "    lev        float64 500.0\n",
      "  * lon        (lon) float64 0.0 0.625 1.25 1.875 ... 118.1 118.8 119.4 120.0\n",
      "  * lat        (lat) float64 0.0 0.5 1.0 1.5 2.0 ... 48.0 48.5 49.0 49.5 50.0\n",
      "  * time       (time) datetime64[ns] 1980-12-12T09:00:00 ... 2017-02-22T09:00:00\n",
      "    dayofyear  (time) int64 dask.array<chunksize=(4,), meta=np.ndarray>\n",
      "    ar         (time) int64 1 1 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "    location   (time) object 'R01' 'R01' 'R02' 'R02' ... 'R01' 'R01' 'R03' 'R03'\n",
      "Data variables:\n",
      "    H          (time, lat, lon) float64 dask.array<chunksize=(4, 101, 193), meta=np.ndarray>\n",
      "    U          (time, lat, lon) float64 dask.array<chunksize=(4, 101, 193), meta=np.ndarray>\n",
      "    V          (time, lat, lon) float64 dask.array<chunksize=(4, 101, 193), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Trim date range\n",
    "start_date = '1980-12-01'\n",
    "end_date = '2017-02-28'\n",
    "idx = slice(start_date, end_date)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# Select DJF months\n",
    "idx = (merra.time.dt.month >= 12) | (merra.time.dt.month <= 2)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# # Select AR days JUST IN R01\n",
    "# idx = (merra.ar >= 1) & (merra.location == 'R01')\n",
    "# Select AR days in all subregions\n",
    "idx = (merra.ar >= 1)\n",
    "merra_ar = merra.sel(time=idx)\n",
    "\n",
    "# print results\n",
    "print(merra_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of independent AR events:  215\n"
     ]
    }
   ],
   "source": [
    "# Count number of independent AR events\n",
    "\n",
    "years = np.arange(1980, 2018) \n",
    "nyrs = len(years)\n",
    "total_events = 0\n",
    "for k in range(nyrs-1):    \n",
    "    # Extract single DJF season\n",
    "    date1 = \"{}-12-01\".format(years[k])\n",
    "    date2 = \"{}-02-28\".format(years[k+1])\n",
    "    x = merra.ar.sel(time=slice(date1,date2)).values\n",
    "    # Count AR events in that season\n",
    "    tags, tmp = persistence(x)\n",
    "    # Add to running event count\n",
    "    total_events += tmp\n",
    "\n",
    "print(\"Number of independent AR events: \", total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 2min 21s, total: 2min 24s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load merra_ar dataset into memory\n",
    "merra_ar = merra_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 101, 193)\n",
      "(374, 101, 193)\n",
      "(374, 101, 193)\n"
     ]
    }
   ],
   "source": [
    "## Create list of variable arrays\n",
    "# Extratropic variables\n",
    "var1 = merra_ar.U\n",
    "var2 = merra_ar.V\n",
    "var3 = merra_ar.H\n",
    "\n",
    "var_list = [var1, var2, var3]\n",
    "\n",
    "# Check that sizes of arrays match\n",
    "for i, in_array in enumerate(var_list):\n",
    "    # Extract variable as numpy array\n",
    "    var1 = in_array.values\n",
    "    print(var1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans removed success is  True\n",
      "Nans removed success is  True\n",
      "Nans removed success is  True\n",
      "Size of array with missing data removed:  (374, 19490)\n",
      "Size of array without missing data removed:  (374, 19493)\n",
      "CPU times: user 659 ms, sys: 452 ms, total: 1.11 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "var_list = spatial_weights(var_list)\n",
    "\n",
    "## Flatten data \n",
    "var_list = flatten_array(var_list)\n",
    "\n",
    "## Remove nans\n",
    "## outputs two lists of arrays\n",
    "## one without nans removed, and one with nans removed\n",
    "var_list, var_list_nan = remove_nans(var_list)\n",
    "\n",
    "print('Size of array with missing data removed: ', var_list_nan[0].shape)\n",
    "print('Size of array without missing data removed: ', var_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of array with missing data removed:  (374, 19490)\n",
      "Size of array without missing data removed:  (374, 19493)\n",
      "Size of array with missing data removed:  (374, 19493)\n",
      "Size of array without missing data removed:  (374, 19493)\n"
     ]
    }
   ],
   "source": [
    "print('Size of array with missing data removed: ', var_list_nan[1].shape)\n",
    "print('Size of array without missing data removed: ', var_list[1].shape)\n",
    "print('Size of array with missing data removed: ', var_list_nan[2].shape)\n",
    "print('Size of array without missing data removed: ', var_list[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58479, 374)\n",
      "Column means:  nan\n",
      "Column std:  nan\n"
     ]
    }
   ],
   "source": [
    "nvar = 3\n",
    "ntim, npts = var_list[0].shape\n",
    "\n",
    "tmp1 = var_list[0]\n",
    "tmp2 = var_list[1]\n",
    "tmp3 = var_list[2]\n",
    "\n",
    "# Transpose arrays to get [space x time]\n",
    "X1 = tmp1.T\n",
    "X2 = tmp2.T\n",
    "X3 = tmp3.T\n",
    "\n",
    "# Center and standardize by columns\n",
    "x1mean = np.mean(X1, axis=0)\n",
    "x1std = np.std(X1, axis=0)\n",
    "X1s = (X1-x1mean) / x1std\n",
    "\n",
    "x2mean = np.mean(X2, axis=0)\n",
    "x2std = np.std(X2, axis=0)\n",
    "X2s = (X2-x2mean) / x2std\n",
    "\n",
    "x3mean = np.mean(X3, axis=0)\n",
    "x3std = np.std(X3, axis=0)\n",
    "X3s = (X3-x3mean) / x3std\n",
    "\n",
    "# Combine variables into single data matrix Xs\n",
    "Xs = np.empty((nvar*npts,ntim))\n",
    "Xs[0*npts:npts+0,:] = X1s\n",
    "Xs[1*npts:npts*(1+1),:] = X2s\n",
    "Xs[2*npts:(2+1)*npts,:] = X3s\n",
    "print(Xs.shape)\n",
    "\n",
    "# Check that column means=0 and std dev=1\n",
    "test = np.mean(np.mean(Xs, axis=0))\n",
    "print(\"Column means: \", np.round(test,2))\n",
    "test = np.mean(np.std(Xs, axis=0))\n",
    "print(\"Column std: \", np.round(test,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58479, 374)\n",
      "Column means:  nan\n",
      "Column std:  nan\n"
     ]
    }
   ],
   "source": [
    "nvar = 3\n",
    "ntim, npts = var_list[0].shape\n",
    "\n",
    "tmp1 = var_list[0]\n",
    "tmp2 = var_list[1]\n",
    "tmp3 = var_list[2]\n",
    "\n",
    "# Transpose arrays to get [space x time]\n",
    "X1 = tmp1.T\n",
    "X2 = tmp2.T\n",
    "X3 = tmp3.T\n",
    "\n",
    "# Combine variables into single data matrix Xs\n",
    "Xs = np.empty((nvar*npts,ntim))\n",
    "Xs[0*npts:npts+0,:] = X1\n",
    "Xs[1*npts:npts*(1+1),:] = X2\n",
    "Xs[2*npts:(2+1)*npts,:] = X3\n",
    "print(Xs.shape)\n",
    "\n",
    "# Center and standardize by columns\n",
    "x1mean = np.mean(Xs, axis=0)\n",
    "x1std = np.std(Xs, axis=0)\n",
    "X1s = (Xs-x1mean) / x1std\n",
    "\n",
    "# Check that column means=0 and std dev=1\n",
    "test = np.mean(np.mean(Xs, axis=0))\n",
    "print(\"Column means: \", np.round(test,2))\n",
    "test = np.mean(np.std(Xs, axis=0))\n",
    "print(\"Column std: \", np.round(test,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_arrays_new(arr_list, mode='t', dispersion_matrix='cor'):\n",
    "    '''standardize variables then put in single flattened array\n",
    "     \n",
    "     Parameters\n",
    "     ----------\n",
    "     arr_list : list\n",
    "        list of variable arrays\n",
    "     \n",
    "     mode : str\n",
    "         mode of EOF - t or s\n",
    "     \n",
    "     dispersion_matrix : str\n",
    "         type of dispersion matrix - cor or cov\n",
    "         \n",
    "     Returns\n",
    "     -------\n",
    "     X : single data matrix with all variables stacked\n",
    "        arrays are standardized by the mode and dispersion matrix types\n",
    "     \n",
    "     ''' \n",
    "    print('EOF mode: ', mode)\n",
    "    print('Dispersion Matrix: ', dispersion_matrix)\n",
    "    nvar = len(arr_list)\n",
    "    ntim, npts = arr_list[0].shape\n",
    "    \n",
    "    # empty flat array to put variables in\n",
    "    if mode == 't':\n",
    "        Xs = np.empty((nvar*npts,ntim))\n",
    "    else: # mode is s\n",
    "        Xs = np.empty((ntim, nvar*npts))\n",
    "    \n",
    "    for i, var1 in enumerate(arr_list):\n",
    "        # if t-mode\n",
    "        if mode == 't':\n",
    "            # transpose to [space x time]\n",
    "            X1 = var1.T\n",
    "            # Standardize by columns and remove column mean for ALL variables\n",
    "            x1mean = np.mean(X1, axis=0)\n",
    "            x1std = np.std(X1, axis=0)\n",
    "            if dispersion_matrix == 'cor':\n",
    "                # Standardize by columns (if correlation)\n",
    "                # remove column mean\n",
    "                X = (X1 - x1mean) / x1std\n",
    "            else: ## dispersion matrix == cov (covariance)\n",
    "                # remove column mean\n",
    "                X = (X1 - x1mean)\n",
    "            \n",
    "            # Combine variables into single data matrix Xs\n",
    "            Xs[i*npts:(i+1)*npts,:] = X\n",
    "        \n",
    "        # if s-mode\n",
    "        else:\n",
    "            # keep array as [time x space]\n",
    "            X1 = var1\n",
    "            # Standardize by columns and remove column mean for ALL variables\n",
    "            x1mean = np.mean(X1, axis=0)\n",
    "            x1std = np.std(X1, axis=0)\n",
    "            if dispersion_matrix == 'cor':\n",
    "                # Standardize by columns (if correlation)\n",
    "                # remove column mean\n",
    "                X = (X1 - x1mean) / x1std\n",
    "            else: ## dispersion matrix == cov (covariance)\n",
    "                # remove column mean\n",
    "                X = (X1 - x1mean)\n",
    "        \n",
    "            # Combine variables into single data matrix Xs\n",
    "            Xs[:, i*npts:(i+1)*npts] = X\n",
    "                       \n",
    "    print(Xs.shape)\n",
    "\n",
    "    # Check that column means=0 and std dev=1\n",
    "    test = np.mean(np.mean(Xs, axis=0))\n",
    "    print(\"Column means: \", np.round(test,2))\n",
    "    test = np.mean(np.std(Xs, axis=0))\n",
    "    print(\"Column std: \", np.round(test,2))\n",
    "    \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOF mode:  t\n",
      "Dispersion Matrix:  cov\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (19493,374) into shape (19490,374)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-202241b5bd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_arrays_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meofmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispersion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cov'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Xs = standardize_arrays(var_list, mode=eofmode, dispersion_matrix=dispmat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-67670cf4c556>\u001b[0m in \u001b[0;36mstandardize_arrays_new\u001b[0;34m(arr_list, mode, dispersion_matrix)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Combine variables into single data matrix Xs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mXs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnpts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnpts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# if s-mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (19493,374) into shape (19490,374)"
     ]
    }
   ],
   "source": [
    "Xs = standardize_arrays_new(var_list_nan, mode=eofmode, dispersion_matrix='cov')\n",
    "\n",
    "# Xs = standardize_arrays(var_list, mode=eofmode, dispersion_matrix=dispmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/repositories/AR_types/modules/eofs.py\u001b[0m in \u001b[0;36mcalc_eofs\u001b[0;34m(z, mode)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# results in [time x time]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# Eigenvector decomposition of R\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Compute covariance/correlation matix [R]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/anaconda3/envs/ar_types/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0m_assert_stacked_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m     \u001b[0m_assert_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/anaconda3/envs/ar_types/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_finite\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array must not contain infs or NaNs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_empty_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute eigenvalues & eigenvectors\n",
    "R, evals, evecs = calc_eofs(z=Xs, mode=eofmode)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(R)\n",
    "plt.colorbar()\n",
    "title = 'Dispersion Matrix (' + dispmat + ')'\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2: Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot data\n",
    "xvals = np.arange(neofs) + 1\n",
    "ax.bar(xvals, pctvar[0:neofs], yerr=err[0:neofs], \n",
    "       color='tab:blue', alpha=0.8)\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel('EOF')\n",
    "ax.set_xticks(xvals)\n",
    "\n",
    "# y-axis\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "yticks = np.arange(0,16,3)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticks) \n",
    "\n",
    "# save fig\n",
    "filepath = path_to_figs + 'exp_variance_' + fname_id + '.png'\n",
    "plt.savefig(filepath, dpi=300)\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neofs = 19\n",
    "loads = loadings(evals, evecs, neofs)\n",
    "\n",
    "print(loads.shape)\n",
    "print(np.round(loads,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eigenvalues, eigenvectors, and loadings\n",
    "\n",
    "neofs = 4   # number of EOFs to save (evecs, loadings3)\n",
    "\n",
    "outfile = path_to_out + 'eigenvalues_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "outfile = path_to_out + 'eigenvectors_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, evecs[:,0:neofs], fmt='%.5f', delimiter=',')\n",
    "\n",
    "outfile = path_to_out + 'loadings_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate principal components (spatial modes)\n",
    "neofs = 19\n",
    "pcs = calc_pcs(Xs, evecs, neofs, mode=eofmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pcs into separate arrays for each variable\n",
    "ntim, nlat, nlon = var2.shape\n",
    "npts = nlat*nlon\n",
    "nvar = len(var_list)\n",
    "# Reshape spatial dim back to 2D map\n",
    "pcmodes = var_list\n",
    "for i in np.arange(len(var_list)):\n",
    "    tmp = pcs[:,i*npts:(i+1)*npts]\n",
    "    pcmodes[i] = np.reshape(tmp, (neofs,nlat,nlon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3: Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Plot of Spatial Modes\n",
    "\n",
    "# number of eofs to plot\n",
    "neofs = 4\n",
    "\n",
    "# Data for plotting extratropics\n",
    "lons = var2.lon.data\n",
    "lats = var2.lat.data\n",
    "udat = pcmodes[0][0:neofs,:,:]\n",
    "vdat = pcmodes[1][0:neofs,:,:]\n",
    "data = pcmodes[2][0:neofs,:,:]\n",
    "\n",
    "print(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "pc_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    pc_label.append(\"PC{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = path_to_figs + 'eofs_'+ fname_id + '.png'\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='2.5%',\n",
    "                label_mode='')\n",
    "\n",
    "#newcmap = cmocean.tools.crop_by_percent(cmo.matter, 15, which='max', N=None)\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "#     ax = draw_basemap(ax, extent=None, xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Add contour fill plot for extratropics\n",
    "    clevs = np.arange(-55,60,5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    # add vectors for extratropics\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=20) \n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('m', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,11))\n",
    "fig.dpi = 200\n",
    "fname = path_to_figs + 'pc_'+ fname_id\n",
    "fmt = 'png'\n",
    "results = evecs[:,:4]\n",
    "\n",
    "X, nplots = results.shape\n",
    "x = np.arange(len(results))\n",
    "\n",
    "for i in np.arange(nplots):\n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    ax.plot(x, results[:,i], '-')\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.set_ylim(-0.2, 0.2)\n",
    "    ax.set_ylabel('Normalized Units')\n",
    "    # subtitles\n",
    "    ax.set_title(pc_label[i], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[i], loc='right', fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.35, wspace=0.003)\n",
    "\n",
    "fig.savefig('%s.%s' %(fname, fmt), bbox_inches='tight', dpi=fig.dpi)\n",
    "fig.clf()\n",
    "\n",
    "\n",
    "plotFile = fname + '.png'\n",
    "print(plotFile)\n",
    "display(Image(plotFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal K\n",
    "\n",
    "# maximum number of clusters (number of iterations)\n",
    "kmax =15\n",
    "# number of eofs\n",
    "neofs = 4\n",
    "# input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Elbow plot\n",
    "outfile = path_to_figs + 'elbow'+ fname_id\n",
    "plot_optimal_k(xdata, kmax, filename=outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of days in each cluster\n",
    "klabels, counts = np.unique(ar_cat, return_counts=True)\n",
    "\n",
    "# Save counts to txt file\n",
    "res = np.column_stack((klabels,counts))\n",
    "headstr = 'AR_TYPE, COUNT'\n",
    "outfile = path_to_out + fname_id + 'k_counts.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%d', header=headstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids (nclust x neofs)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Save centroids to txt file\n",
    "res = np.column_stack((klabels,centroids))\n",
    "headstr = \"AR_TYPE, EOF1, EOF2, EOF3, EOF4\"\n",
    "outfile = path_to_out + fname_id + 'centroids.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%s', header=headstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save AR Category Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save AR location, loadings (EOF1-4), and category label (AR days only)\n",
    "\n",
    "# Vector of AR dates\n",
    "dates_arDays = era_ar.time.values\n",
    "\n",
    "# Create new dataframe\n",
    "data = {'LOC':era_ar.location.values,\n",
    "        'EOF1':loads[:,0],\n",
    "        'EOF2':loads[:,1],\n",
    "        'EOF3':loads[:,2],\n",
    "        'EOF4':loads[:,3],\n",
    "        'AR_CAT':ar_cat}\n",
    "df_out = pd.DataFrame(data, index=dates_arDays)\n",
    "print(df_out)\n",
    "\n",
    "# Export dataframe as csv\n",
    "outfile = path_to_out + fname_id + 'hma_AR-types-loadings.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save time series of all DJF days with AR types\n",
    "\n",
    "# Arrays with ALL DJF days\n",
    "dates_allDays = era.time.values\n",
    "ar_cat_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "\n",
    "# Loop over ar days and match to ar_full \n",
    "for i, date in enumerate(dates_arDays):\n",
    "    idx = np.where(dates_allDays == date)\n",
    "    ar_cat_allDays[idx] = ar_cat[i]  \n",
    "\n",
    "# Create dataframe\n",
    "data = {'AR_CAT':ar_cat_allDays}\n",
    "df_out = pd.DataFrame(data, index=dates_allDays)\n",
    "print(df_out)\n",
    "\n",
    "outfile = path_to_out + fname_id + 'hma_AR-types-djf.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ar_types)",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
