{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AR non-anomaly composites of different variables using different cEOF tests (e.g. DJF, MAM, DJF-MAM). \n",
    "\n",
    "Creates a 9 panel plot with the following variables in the columns\n",
    "1. (upper) 250-hPa geopotential heights (contour lines), isotachs (contour shading), and wind vectors\n",
    "2. (ivt) \n",
    "3. (precip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import os, sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.colors import ListedColormap\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "\n",
    "# Extra \n",
    "from scipy.ndimage import gaussian_filter    # smoothing contour lines\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# import personal modules\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap, add_subregion_boxes, make_cmap, add_hasia_labels\n",
    "import nclcmaps as nclc\n",
    "from utils import load_era5_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/sbarc/students/nash/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/'      # figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text \n",
    "rcParams['font.family'] = 'sans-serif'   # font family = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # default sans-serif font = 'Arial'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose season\n",
    "ssn = 'djfmam'\n",
    "ceofid = 'HUV500_10ARSUB'\n",
    "neofs = 4 # choose number of eofs\n",
    "nk = 4 # choose number of clusters\n",
    "\n",
    "out_path = path_to_out + ceofid + '/' + ssn + '/' + 'neof' + str(neofs) + '/k' + str(nk) + '/'\n",
    "fig_path = path_to_figs + ceofid + '/' + ssn + '/'+ 'neof' + str(neofs) + '/k' + str(nk) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configuration file for dictionary choice\n",
    "yaml_doc = '../data/plt_config.yml'\n",
    "\n",
    "config = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "#select dictionaries - choose var, anom/nanom, and season\n",
    "# upper_ precip_ ivt_ and non_anom anom\n",
    "plot_dict_upper = config['upper_non_anom']\n",
    "plot_dict_ivt = config['ivt_non_anom']\n",
    "plot_dict_prec = config['precip_non_anom']\n",
    "\n",
    "plot_dicts = [plot_dict_upper, plot_dict_ivt, plot_dict_prec]\n",
    "\n",
    "# djf_dict mam_dict djfmam_dict\n",
    "ar_dict = config[ssn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERA5 renalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    '''\n",
    "    keep only selected lats and lons and rename vars\n",
    "    '''\n",
    "    if pathvar == 'huvq':\n",
    "        rename_dict = {'z': 'H', \n",
    "                       'u': 'U', \n",
    "                       'v':'V', \n",
    "                       'q': 'QV',\n",
    "                       'latitude': 'lat',\n",
    "                       'longitude': 'lon'}\n",
    "        subset = ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), level=lev)\n",
    "        subset = subset.rename(rename_dict)\n",
    "\n",
    "    if pathvar == 'ivt':\n",
    "        rename_dict = {'p71.162': 'ivte', \n",
    "                       'p72.162': 'ivtn', \n",
    "                       'latitude': 'lat',\n",
    "                       'longitude': 'lon'}\n",
    "        subset = ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax))\n",
    "        subset = subset.rename(rename_dict)\n",
    "\n",
    "    if pathvar == 'prec':\n",
    "        rename_dict = {'mtpr': 'prec', \n",
    "                       'latitude': 'lat',\n",
    "                       'longitude': 'lon'}\n",
    "\n",
    "        subset = ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax))\n",
    "        subset = subset.rename(rename_dict)\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lat/lon grid \n",
    "bbox = [20, 110, 0, 55]\n",
    "lonmin, lonmax, latmin, latmax = bbox\n",
    "# select vars and levs\n",
    "varlst = ['huvq', 'ivt', 'prec']\n",
    "levlst = [250., None, None]\n",
    "\n",
    "f = []\n",
    "\n",
    "for i, (pathvar, lev) in enumerate(zip(varlst, levlst)):\n",
    "    filepath_pattern = path_to_data + 'ERA5/{0}/daily/out.era5_hma_*.nc'.format(pathvar)\n",
    "    f.append(xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords'))\n",
    "\n",
    "# merge huvq and ivt\n",
    "ds1 = xr.merge(f[:2], compat='override')\n",
    "# keep prec separate\n",
    "ds2 = f[2]\n",
    "# convert geopotential to geopotential height (m)\n",
    "# ds1 = ds1.assign(h=lambda ds: ds.h/(9.80665))\n",
    "# /(9.80665))), # convert to geopotential height (m)\n",
    "# convert prec units to mm/day\n",
    "# ds2 = ds2.assign(prec=lambda ds: ds.prec*86400)\n",
    "ds_lst = [ds1, ds2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nash/DATA/repositories/AR_types/out/HUV500_10ARSUB/djfmam/neof4/k4/AR-types_ALLDAYS.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ced8491487b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'AR-types_ALLDAYS.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nash/DATA/repositories/AR_types/out/HUV500_10ARSUB/djfmam/neof4/k4/AR-types_ALLDAYS.csv'"
     ]
    }
   ],
   "source": [
    "filepath = out_path + 'AR-types_ALLDAYS.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ds in enumerate(ds_lst):\n",
    "    # Trim date range\n",
    "    idx = slice(ar_dict['start_date'], ar_dict['end_date'])\n",
    "    ds = ds.sel(time=idx)\n",
    "    \n",
    "    # Select months\n",
    "    if ar_dict['mon_s'] > ar_dict['mon_e']:\n",
    "        idx = (ds.time.dt.month >= ar_dict['mon_s']) | (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "    else:\n",
    "        idx = (ds.time.dt.month >= ar_dict['mon_s']) & (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "    ds = ds.sel(time=idx)\n",
    "    \n",
    "    # Combine AR Cat data w/ reanalysis data\n",
    "    # Add ar time series to the ERA dataset\n",
    "    ds['ar'] = ('time', df.AR_CAT)\n",
    "    ds = ds.set_coords('ar')\n",
    "    \n",
    "    ds_lst[i] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute AR Composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_ar_comp = []\n",
    "for i, ds in enumerate(ds_lst):\n",
    "    idx = (ds.ar >= 1)\n",
    "    # select AR days\n",
    "    era_ar = ds.sel(time=idx)\n",
    "    # Compute composites of each AR type\n",
    "    era_ar_comp.append(era_ar.groupby('ar').mean('time').compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Plot Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write precip composites to data folder\n",
    "# era_ar_comp[1].to_netcdf(path=path_to_out+'precip_djfmam_h500_eof2_k3.nc', mode = 'w', format='NETCDF4')\n",
    "# era_ar_comp[0].to_netcdf(path=path_to_out+'huv_ivt_djfmam_h500_eof2_k3.nc', mode = 'w', format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn plot style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# Set projections\n",
    "datacrs = ccrs.PlateCarree()   # data/source\n",
    "mapcrs = ccrs.PlateCarree()    # map/destination\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+10,10)\n",
    "dy = np.arange(latmin,latmax+10,10)\n",
    "\n",
    "# subregion info \n",
    "# [ymin, xmin]\n",
    "sr_xy = [[65, 30], [75, 25], [85, 20]]\n",
    "# width of subregion\n",
    "sr_width = [10, 10, 10]\n",
    "# height of subregion\n",
    "sr_height = [10, 10, 10]\n",
    "\n",
    "# 9-panel plot    \n",
    "panel_9 = {'nrows': 3,\n",
    "            'ncols': 2,\n",
    "            'artype_lst': list(range(1, nk+1)),\n",
    "            'panel': '9pan'}\n",
    "\n",
    "# # test plot    \n",
    "# panel_test = {'nrows': nk,\n",
    "#             'ncols': 3,\n",
    "#             'artype_lst': list(range(1, nk+1)),\n",
    "#             'panel': 'test'+str(neofs)+str(nk)}\n",
    "\n",
    "\n",
    "panel_dict = panel_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dict = panel_dict\n",
    "filepath = fig_path + 'composite_' + fig_dict['panel'] + '.png'\n",
    "nrows = nk\n",
    "ncols = fig_dict['ncols']\n",
    "artype_lst = fig_dict['artype_lst']\n",
    "row_lbl = ['AR Type 1', 'AR Type 2', 'AR Type 3']\n",
    "col_lbl1 = ['(a)', '(b)', '(c)']\n",
    "col_lbl2 = ['(d)', '(e)', '(f)']\n",
    "col_lbl3 = ['(h)', '(i)', '(j)']\n",
    "\n",
    "ext1 = [20, 110, 5, 55]\n",
    "ext2 = [60, 105, 20, 45]\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, [0.1, 0.1, 0.66, 0.8], axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, 2), axes_pad = 0.45,\n",
    "                cbar_location='bottom', cbar_mode='edge',\n",
    "                cbar_pad=0.25, cbar_size='7%',label_mode='',\n",
    "                direction='column',\n",
    "                aspect=True,\n",
    "                share_all = False)\n",
    "\n",
    "axgr2 = AxesGrid(fig, [0.7125, 0.1, 0.33, 0.8], axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, 1), axes_pad = 0.45,\n",
    "                cbar_location='bottom', cbar_mode='edge',\n",
    "                cbar_pad=0.25, cbar_size='7%',label_mode='',\n",
    "                direction='column',\n",
    "                aspect=True,\n",
    "                share_all = False)\n",
    "\n",
    "################################\n",
    "########## IVT PLOTS ###########\n",
    "################################\n",
    "for k, (ax, ar_type) in enumerate(zip(axgr[0:nrows], artype_lst)):\n",
    "    data = era_ar_comp[0].sel(ar=ar_type)\n",
    "    plot_dict = plot_dict_ivt\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values    \n",
    "    ax = draw_basemap(ax, extent=ext1, xticks=dx, yticks=dy, left_lats=False, right_lats=True)\n",
    "    # Contour Filled\n",
    "        \n",
    "    # IVT (filled contour)\n",
    "    uvec = data.ivte.values\n",
    "    vvec = data.ivtn.values\n",
    "    ivt = np.sqrt(uvec**2 + vvec**2)\n",
    "    cflevs = np.arange(plot_dict['cflevs'][0], plot_dict['cflevs'][1], plot_dict['cflevs'][2])\n",
    "    cf = ax.contourf(lons, lats, ivt, transform=datacrs,\n",
    "                     levels=cflevs, cmap=plot_dict['cmap'], alpha=0.9, extend='max') \n",
    "        \n",
    "    # Wind barbs / vectors\n",
    "    Q = ax.quiver(lons, lats, uvec, vvec, transform=datacrs, \n",
    "                  color='k', regrid_shape=15, pivot='middle',\n",
    "                  angles='xy', scale_units='xy', scale=plot_dict['quiver_scale'], units='xy')\n",
    "    \n",
    "    # quiver key\n",
    "    qk = ax.quiverkey(Q, 0.7, 1.07, plot_dict['quiver_key'], plot_dict['quiver_key_lbl'], labelpos='E',\n",
    "                          coordinates='axes', fontproperties={'size': 8.0})\n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(col_lbl1[k], loc='left',fontsize=13)\n",
    "    \n",
    "# Colorbar (single)\n",
    "    cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "    cb.set_label(plot_dict['cb_label'])\n",
    "    \n",
    "#################################\n",
    "########## UPPER PLOTS ##########\n",
    "#################################\n",
    "for k, (ax, ar_type) in enumerate(zip(axgr[nrows:nrows+nrows], artype_lst)):\n",
    "    data = era_ar_comp[0].sel(ar=ar_type)\n",
    "    plot_dict = plot_dict_upper\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values    \n",
    "    ax = draw_basemap(ax, extent=ext1, xticks=dx, yticks=dy, left_lats=False, right_lats=True)\n",
    "    # Contour Filled\n",
    "\n",
    "    # 250-hPa Winds (m/s)\n",
    "    uwnd = data.U.values * units('m/s')\n",
    "    vwnd = data.V.values * units('m/s')\n",
    "    wspd = mpcalc.wind_speed(uwnd, vwnd)\n",
    "\n",
    "    cflevs = np.arange(plot_dict['cflevs'][0], plot_dict['cflevs'][1], plot_dict['cflevs'][2])\n",
    "    cf = ax.contourf(lons, lats, wspd, transform=datacrs,\n",
    "                     levels=cflevs, cmap=plot_dict['cmap'], alpha=0.9, extend='max')\n",
    "    \n",
    "    # Contour Lines\n",
    "\n",
    "    # 250-hPa Heights\n",
    "    hgts = data.H.values/(90.80665) # convert to geopotential height (decimeters)\n",
    "    clevs = np.arange(plot_dict['clevs'][0], plot_dict['clevs'][1], plot_dict['clevs'][2])\n",
    "    cs = ax.contour(lons, lats, hgts, transform=datacrs,\n",
    "                    levels=clevs, colors='k', linewidths=1.1)\n",
    "    plt.clabel(cs, fmt='%d',fontsize=8.5, inline_spacing=5)  \n",
    "        \n",
    "    # Wind barbs / vectors\n",
    "    Q = ax.quiver(lons, lats, uwnd, vwnd, transform=datacrs, \n",
    "                  color='k', regrid_shape=15, pivot='middle',\n",
    "                  angles='xy', scale_units='xy', scale=plot_dict['quiver_scale'], units='xy')\n",
    "    \n",
    "    # quiver key\n",
    "    qk = ax.quiverkey(Q, 0.7, 1.07, plot_dict['quiver_key'], plot_dict['quiver_key_lbl'], labelpos='E',\n",
    "                     coordinates='axes', fontproperties={'size': 8.0})\n",
    "    \n",
    "    # subtitles\n",
    "#     plt_label = 'AR Type {0}'.format(ar_type)\n",
    "    ax.set_title(col_lbl2[k], loc='left',fontsize=13)\n",
    "#     # Row labels\n",
    "#     ax.text(-0.07, 0.55, row_lbl[k], va='bottom', ha='center',\n",
    "#         rotation='vertical', rotation_mode='anchor', fontsize=13,\n",
    "#         transform=ax.transAxes)\n",
    "    \n",
    "# Colorbar (single edge)\n",
    "    cb = fig.colorbar(cf, axgr.cbar_axes[1], orientation='horizontal', drawedges=True)\n",
    "    cb.set_label(plot_dict['cb_label'])\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "########## PREC PLOTS ##########\n",
    "################################\n",
    "\n",
    "for k, (ax, ar_type) in enumerate(zip(axgr2, artype_lst)):\n",
    "    data = era_ar_comp[1].sel(ar=ar_type)\n",
    "    plot_dict = plot_dict_prec\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values    \n",
    "    ax = draw_basemap(ax, extent=ext2, xticks=dx, yticks=dy, left_lats=False, right_lats=True)\n",
    "    # Contour Filled\n",
    "\n",
    "    # Precip (filled contour)\n",
    "    cflevs = np.arange(plot_dict['cflevs'][0], plot_dict['cflevs'][1], plot_dict['cflevs'][2])\n",
    "    cmap = nclc.cmap('WhiteBlueGreenYellowRed')\n",
    "#     cmap = 'Blues'\n",
    "    prec = data.prec.values*86400 # convert to mm/day\n",
    "#     print(prec.min(), prec.max())\n",
    "    cf = ax.contourf(lons, lats, prec, transform=datacrs,\n",
    "                     levels=cflevs, cmap=cmap, alpha=0.9, extend='max')\n",
    "    # add hasia labels\n",
    "#     add_hasia_labels(ax=ax, fntsize=5)\n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(col_lbl3[k], loc='left',fontsize=13)\n",
    "    \n",
    "# # Colorbar (single)\n",
    "    cb = fig.colorbar(cf, axgr2.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "    cb.set_label(plot_dict['cb_label'])\n",
    "    \n",
    "# Save figure\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clevs = np.arange(100, 275, 50)\n",
    "clevs\n",
    "\n",
    "cmap_data = [\n",
    "             (0, 160, 255), # light blue 200-300\n",
    "             (230, 220, 50), # yellow 800-900\n",
    "             (250, 60, 60), # light red 2000-2100\n",
    "            ]\n",
    "\n",
    "my_cmap = make_cmap(cmap_data, bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dict = panel_dict\n",
    "filepath = fig_path + 'composite_schematic.png'\n",
    "nrows = nk\n",
    "ncols = 1\n",
    "artype_lst = fig_dict['artype_lst']\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, 111, axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), axes_pad = 0.45,\n",
    "                cbar_location='bottom', cbar_mode='single',\n",
    "                cbar_pad=0.05, cbar_size='7%',label_mode='',\n",
    "                direction='column')\n",
    "\n",
    "for k, (ax, ar_type) in enumerate(zip(axgr, artype_lst)):\n",
    "    data = era_ar_comp[0].sel(ar=ar_type)\n",
    "    plot_dict = plot_dict_ivt\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values    \n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Contour Filled\n",
    "        \n",
    "    # IVT (filled contour)\n",
    "    uvec = data.ivte.values\n",
    "    vvec = data.ivtn.values\n",
    "    ivt = np.sqrt(uvec**2 + vvec**2)\n",
    "    ivt_sm = gaussian_filter(ivt, sigma=1.5)\n",
    "    cflevs = np.arange(125, 510, 125)\n",
    "    cf = ax.contourf(lons, lats, ivt_sm, transform=datacrs,\n",
    "                     levels=cflevs, cmap=cmo.deep, alpha=0.9, extend='neither') \n",
    "        \n",
    "    # Wind barbs / vectors\n",
    "    Q = ax.quiver(lons, lats, uvec, vvec, transform=datacrs, \n",
    "                  color='k', regrid_shape=17, pivot='middle',\n",
    "                  angles='xy', scale_units='xy', scale=plot_dict['quiver_scale'], units='xy')\n",
    "    \n",
    "    # quiver key\n",
    "    qk = ax.quiverkey(Q, 0.7, 1.07, plot_dict['quiver_key'], plot_dict['quiver_key_lbl'], labelpos='E',\n",
    "                          coordinates='axes', fontproperties={'size': 8.0})\n",
    "    \n",
    "    # Contour Lines\n",
    "    # 250-hPa Heights\n",
    "    hgts = data.H.values/(90.80665) # convert to geopotential height (decimeters)\n",
    "    clevs = np.arange(840, 1280, 12)\n",
    "    cs = ax.contour(lons, lats, hgts, transform=datacrs,\n",
    "                    levels=clevs, colors='grey', linewidths=0.7, linestyles='solid')\n",
    "    plt.clabel(cs, fmt='%d',fontsize=8.5, inline_spacing=5) \n",
    "    \n",
    "    # Contour Lines\n",
    "    # Precip\n",
    "    data = era_ar_comp[1].sel(ar=ar_type)\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values\n",
    "    prec = data.prec.values*86400 # convert to mm/day\n",
    "    prec_sm = gaussian_filter(prec, sigma=2.0)\n",
    "    clevs = np.arange(5, 30, 10)\n",
    "    cs = ax.contour(lons, lats, prec_sm, transform=datacrs,\n",
    "                    levels=clevs, colors='k', linewidths=1.5)\n",
    "    \n",
    "    \n",
    "    # subtitles\n",
    "    plt_label = 'AR Type {0}'.format(ar_type)\n",
    "    ax.set_title(plt_label, loc='left',fontsize=13)\n",
    "    # add subregions\n",
    "    add_subregion_boxes(ax=ax, subregion_xy=sr_xy, width=sr_width, \n",
    "                        height=sr_height, ecolor='red', datacrs=datacrs)\n",
    "    \n",
    "# Colorbar (single)\n",
    "    cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "    cb.set_label('kg m$^{-1}$ s$^{-1}$')\n",
    "    \n",
    "# Save figure\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dict = panel_dict\n",
    "filepath = fig_path + 'composite_precip.png'\n",
    "nrows = nk\n",
    "ncols = 1\n",
    "artype_lst = fig_dict['artype_lst']\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, 111, axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), axes_pad = 0.45,\n",
    "                cbar_location='bottom', cbar_mode='single',\n",
    "                cbar_pad=0.05, cbar_size='7%',label_mode='',\n",
    "                direction='column')\n",
    "\n",
    "for k, (ax, ar_type) in enumerate(zip(axgr, artype_lst)):\n",
    "    plot_dict = plot_dict_prec\n",
    "    # Precip\n",
    "    data = era_ar_comp[1].sel(ar=ar_type)\n",
    "    # lat/lon arrays\n",
    "    lats = data.lat.values\n",
    "    lons = data.lon.values\n",
    "    prec = data.prec.values*86400 # convert to mm/day   \n",
    "    ax = draw_basemap(ax, extent=[60, 105, 20, 45], xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Precip (filled contour)\n",
    "    cflevs = np.arange(plot_dict['cflevs'][0], plot_dict['cflevs'][1], plot_dict['cflevs'][2])\n",
    "    cmap = nclc.cmap('WhiteBlueGreenYellowRed')\n",
    "#     cmap = 'Blues'\n",
    "    prec = data.prec.values\n",
    "    print(prec.min(), prec.max())\n",
    "    cf = ax.contourf(lons, lats, prec, transform=datacrs,\n",
    "                     levels=cflevs, cmap=cmap, alpha=0.9, extend='max')\n",
    "    # add hasia labels\n",
    "    add_hasia_labels(ax=ax, fntsize=5)\n",
    "    \n",
    "    # Contour Lines\n",
    "    clevs = np.arange(5, 30, 5)\n",
    "    prec_sm = gaussian_filter(prec, sigma=2.0)\n",
    "    cs = ax.contour(lons, lats, prec_sm, transform=datacrs,\n",
    "                    levels=clevs, colors='k', linewidths=1.25)\n",
    "    \n",
    "    \n",
    "    # subtitles\n",
    "    plt_label = 'AR Type {0}'.format(ar_type)\n",
    "    ax.set_title(plt_label, loc='left',fontsize=13)\n",
    "    \n",
    "# Colorbar (single)\n",
    "    cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "    cb.set_label('m s$^{-1}$')\n",
    "    \n",
    "# Save figure\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hasia2)",
   "language": "python",
   "name": "hasia2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
