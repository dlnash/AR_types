{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import os, sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.ticker as mticker\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "\n",
    "# Extra \n",
    "from scipy.ndimage import gaussian_filter    # smoothing contour lines\n",
    "from scipy.stats import linregress\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# import personal modules\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence, select_months\n",
    "from teleconnections import build_teleconnection_df\n",
    "from statistical_tests import lin_regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up multiple workers for use later when parallel\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=True, workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/nash/DATA/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/'      # figures\n",
    "\n",
    "# USE pandas.options TO DISPLAY FLOATS TO TWO DECIMAL PLACES\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1979-03-01'\n",
    "end_date = '2018-05-31'\n",
    "\n",
    "tele = build_teleconnection_df('daily', 'ANOM', start_date, end_date)\n",
    "tele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_djf = select_months(tele, 12, 2)\n",
    "df_index_mam = select_months(tele, 3, 5)\n",
    "print('# DJF days: ', len(df_index_djf))\n",
    "print('# MAM days: ', len(df_index_mam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_daily_df(ssn, nk):\n",
    "    fname_id = 'HUV500t0120050cor'\n",
    "    filepath = path_to_out + fname_id + 'hma_AR-types-' + ssn + '.csv'\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # set up datetime index\n",
    "    df = df.rename(columns={'Unnamed: 0': 'date'})\n",
    "    df = df.set_index(pd.to_datetime(df.date))\n",
    "    \n",
    "    ## Break up columns into different AR Types\n",
    "    keys = []\n",
    "    for k in range(nk):\n",
    "        keys.append(\"AR_CAT{:1d}\".format(k+1,))\n",
    "\n",
    "    values = np.zeros((len(df.index)))\n",
    "    dicts = dict(zip(keys, values))\n",
    "\n",
    "    df_cat = pd.DataFrame(dicts, index=df.index)\n",
    "\n",
    "    for k in range(nk):\n",
    "        idx = (df['AR_CAT'] == k+1)\n",
    "        col = \"AR_CAT{:1d}\".format(k+1,)\n",
    "        df_cat.loc[idx, col] = 1\n",
    "        \n",
    "    # get total of all AR types\n",
    "    df_cat['AR_ALL'] = df_cat['AR_CAT1'] + df_cat['AR_CAT2'] + df_cat['AR_CAT3']\n",
    "    df_cat['AR_CAT'] = df['AR_CAT']\n",
    "    \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_djf = ar_daily_df(ssn='djf', nk=3)\n",
    "df_mam = ar_daily_df(ssn='mam', nk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine ar df with tele df\n",
    "print(len(df_djf), len(df_index_djf), len(df_mam), len(df_index_mam))\n",
    "# join indices with AR count\n",
    "new_djf = df_djf.join(df_index_djf)\n",
    "new_mam = df_mam.join(df_index_mam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select lat/lon grid\n",
    "lonmin = -180\n",
    "lonmax = 180\n",
    "latmin = 0\n",
    "latmax = 90\n",
    "\n",
    "def preprocess(ds):\n",
    "    '''keep only selected lats and lons'''\n",
    "    return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax))\n",
    "\n",
    "# # open HUV data\n",
    "filepath_pattern = path_to_data + 'ERA5/huv/anomalies/daily_filtered_anomalies_*.nc'  \n",
    "    \n",
    "f2 = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "\n",
    "ds = f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update season for djf or mam plot\n",
    "ssn = 'mam'\n",
    "\n",
    "if ssn == 'djf':\n",
    "    start_date = '1979-12-01'\n",
    "    end_date = '2018-02-28'\n",
    "    mon_s = 12\n",
    "    mon_e = 2\n",
    "    new_ds = new_djf\n",
    "elif ssn == 'mam':\n",
    "    start_date = '1979-03-01'\n",
    "    end_date = '2018-05-31'\n",
    "    mon_s = 3\n",
    "    mon_e = 5\n",
    "    new_ds = new_mam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim date range\n",
    "\n",
    "idx = slice(start_date, end_date)\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# Select months\n",
    "if mon_s > mon_e:\n",
    "    idx = (ds.time.dt.month >= mon_s) | (ds.time.dt.month <= mon_e)\n",
    "else:\n",
    "    idx = (ds.time.dt.month >= mon_s) & (ds.time.dt.month <= mon_e)\n",
    "    \n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "\n",
    "# Combine AR Cat data w/ reanalysis data\n",
    "# Add ar time series to the ERA dataset\n",
    "cols = ['AR_CAT', 'AO', 'PDO', 'ENSO', 'SH']\n",
    "for i, col in enumerate(cols):\n",
    "    ds[col] = ('time', new_ds[col])\n",
    "\n",
    "ds = ds.set_coords('AR_CAT')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_linregress(x, y):\n",
    "    # Wrapper around scipy linregress to use in apply_ufunc\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    return np.array([slope, intercept, r_value, p_value, std_err])\n",
    "\n",
    "def lin_regress(ds, x, y):\n",
    "    '''Wrapped scipy.stats.linregress to calculate slope, y-int, r-value, p-value, and standard error in xr.dataset form'''\n",
    "    return xr.apply_ufunc(new_linregress, ds[x], ds[y],\n",
    "                           input_core_dims=[['time'], ['time']],\n",
    "                           output_core_dims=[[\"parameter\"]],\n",
    "                           vectorize=True,\n",
    "                           dask=\"parallelized\",\n",
    "                           output_dtypes=['float64'],\n",
    "                           output_sizes={\"parameter\": 5},\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run linear regression for each AR Type and each teleconnection\n",
    "cols = ['AO', 'PDO', 'ENSO', 'SH']\n",
    "rval = []\n",
    "pval = []\n",
    "slope = []\n",
    "\n",
    "for i, artype in enumerate(np.arange(1,4)):\n",
    "    idx = (ds['AR_CAT'] == artype)\n",
    "    data = ds.sel(time=idx).chunk({'time': -1})\n",
    "    for j, tele in enumerate(cols):\n",
    "        results = lin_regress(data, 'z', tele)\n",
    "#         print(artype, tele)\n",
    "#         rval.append(results.isel(parameter=2).values)\n",
    "#         pval.append(results.isel(parameter=3).values)\n",
    "        slope.append(results.isel(parameter=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn plot style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.NorthPolarStereo()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# lat/lon arrays\n",
    "lats = data.latitude.values\n",
    "lons = data.longitude.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = path_to_figs + 'composites/teleconnections/regress_teleconnections_slope_' + ssn + '.png'    \n",
    "nrows = 3\n",
    "ncols = 4\n",
    "\n",
    "cols = ['ao', 'pdo', 'enso', 'sh']\n",
    "plt_lbls = ['AO', 'PDO', 'ENSO', 'SH']+['']*8\n",
    "row_lbls = ['AR Type 1', '', '', '',\n",
    "           'AR Type 2', '', '', '',\n",
    "           'AR Type 3', '', '', '']\n",
    "\n",
    "cmap = cmo.balance\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, 111, axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), axes_pad = 0.2,\n",
    "                cbar_location='bottom', cbar_mode='single',\n",
    "                cbar_pad=0.10, cbar_size='2%',label_mode='',\n",
    "                direction='row')\n",
    "\n",
    "for k, ax in enumerate(axgr): \n",
    "    data = slope[k]\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], grid=True)\n",
    "    # Contour Filled\n",
    "    cflevs = np.arange(-0.1, 0.1, 0.01)\n",
    "    cf = ax.contourf(lons, lats, data, transform=datacrs,\n",
    "                     levels=cflevs, cmap=cmap, alpha=0.9, extend='both')\n",
    "    ax.set_title(plt_lbls[k], fontsize=13)\n",
    "    # Row labels\n",
    "    ax.text(-0.07, 0.55, row_lbls[k], va='bottom', ha='center',\n",
    "        rotation='vertical', rotation_mode='anchor', fontsize=13,\n",
    "        transform=ax.transAxes)\n",
    "                  \n",
    "# # Colorbar (single)\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True, extend='both', spacing='uniform')\n",
    "cb.set_label('m')\n",
    "    \n",
    "# Save figure\n",
    "plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.min(), data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar_types",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
