{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis - testing different configurations to see which cEOF captures the most varaince of precipitation related to AR events\n",
    "\n",
    "  1. cEOF 250 hPa H, U, V in extratropics, 850 hPa Q, U, V in tropics\n",
    "  2. cEOF 500 hPa H, U, V in extratropics, 500 hPa Q, U, V in tropics\n",
    "  3. cEOF 500 hPa H, U, V in extratropics, 500 hPa H, U, V in tropics\n",
    "  4. cEOF 500 hPa H, U, V in full domain [0-120 E, 0-50N]\n",
    "  5. cEOF IVTu and IVTv in full domain\n",
    "  6. cEOF 500 hPa H, U, V and 850 hPa Q in full domain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import cycle\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mticker\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from scipy import stats\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence, select_months\n",
    "from eofs import *\n",
    "from ar_funcs import preprocess_ar_area_subregions\n",
    "from kmeans import *\n",
    "import nclcmaps as ncl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/nash/DATA/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/'      # figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select MERRA2 or ERA5\n",
    "reanalysis = 'era5'\n",
    "\n",
    "if reanalysis == 'era5':\n",
    "    start_date = '1979-01-01'\n",
    "    end_date = '2018-12-31'\n",
    "    filename = 'ar_catalog_v3_ERAI_fraction_HASIAsubregions.nc'\n",
    "## if MERRA2\n",
    "else:\n",
    "    start_date = '1980-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    filename = 'ar_catalog_fraction_HASIAsubregions.nc'\n",
    "    \n",
    "f1 = path_to_data + 'CH1_generated_data/' + filename\n",
    "ds = xr.open_dataset(f1)\n",
    "# Set dates\n",
    "ds = ds.sel(time=slice(start_date, end_date))\n",
    "## Preprocess AR subregions - get dataframe of AR days based on area threshold\n",
    "df = preprocess_ar_area_subregions(df=ds.to_dataframe(), thres=0.3)\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configuration file for season dictionary choice\n",
    "yaml_doc = '../data/config.yml'\n",
    "config1 = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "ssn = 'djfmam'\n",
    "ar_dict = config1[ssn]\n",
    "\n",
    "# import configuration file for ceof dictionary choice\n",
    "## test number - NOTE as of 7/1/2020 sticking with test 4\n",
    "yaml_doc = '../data/ceof_config.yml'\n",
    "config2 = yaml.load(open(yaml_doc), Loader=yaml.SafeLoader)\n",
    "config_dict = config2['test4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set variable names (for saving data/figs)\n",
    "testname = config_dict['name']\n",
    "eofmode = 't' # s or t\n",
    "dispmat = 'cor' # dispersion matrix type correlation/covariance\n",
    "pathvar = config_dict['pathvar']\n",
    "ceofid = config_dict['fnameID']\n",
    "\n",
    "# Select lat/lon grid \n",
    "# Tropics/Extratropics Domain\n",
    "lonmin = config_dict['latlon'][0]\n",
    "lonmax = config_dict['latlon'][1]\n",
    "latmin = config_dict['latlon'][2]\n",
    "latmax = config_dict['latlon'][3]\n",
    "\n",
    "lev = config_dict['levs']\n",
    "\n",
    "out_path = path_to_out + ceofid + '/' + ssn + '/'\n",
    "fig_path = path_to_figs + ceofid + '/' + ssn + '/'\n",
    "\n",
    "if pathvar == 'huvq':\n",
    "    def preprocess(ds):\n",
    "        '''keep only selected lats and lons'''\n",
    "        return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), level=lev)\n",
    "if pathvar == 'ivt':\n",
    "        def preprocess(ds):\n",
    "            '''keep only selected lats and lons'''\n",
    "            return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax))\n",
    "\n",
    "# open anomaly data\n",
    "filepath_pattern = path_to_data + 'ERA5/{0}/anomalies/daily_filtered_anomalies_{0}_*.nc'.format(pathvar)\n",
    "    \n",
    "    \n",
    "f2 = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "\n",
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pathvar == 'huvq':\n",
    "\n",
    "    # Create new dataset to rename lat lon\n",
    "    ds = xr.Dataset({'H': (['time', 'level', 'lat', 'lon'], f2['z'].values),\n",
    "                     'U': (['time', 'level',  'lat', 'lon'], f2['u'].values),\n",
    "                     'V': (['time', 'level',  'lat', 'lon'], f2['v'].values),\n",
    "                     'QV': (['time', 'level',  'lat', 'lon'], f2['q'].values)},\n",
    "                          coords={'time': (['time'], f2.time.values),\n",
    "                                  'level': (['level'], f2.level.values),\n",
    "                                  'lat': (['lat'], f2.latitude.values),\n",
    "                                  'lon': (['lon'], f2.longitude.values)})\n",
    "\n",
    "if pathvar == 'ivt':\n",
    "    ds = xr.Dataset({'ivte': (['time', 'lat', 'lon'], f2['p71.162'].values),\n",
    "                     'ivtn': (['time', 'lat', 'lon'], f2['p72.162'].values)},\n",
    "                          coords={'time': (['time'], f2.time.values),\n",
    "                                  'lat': (['lat'], f2.latitude.values),\n",
    "                                  'lon': (['lon'], f2.longitude.values)})\n",
    "\n",
    "\n",
    "ds\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AR time series to merra; set as coordinate variables\n",
    "ds['ar'] = ('time', df.ar)\n",
    "ds = ds.set_coords('ar')\n",
    "\n",
    "ds['location'] = ('time', df.location)\n",
    "ds = ds.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = slice(ar_dict['start_date'], ar_dict['end_date'])\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# Select months\n",
    "if ar_dict['mon_s'] > ar_dict['mon_e']:\n",
    "    idx = (ds.time.dt.month >= ar_dict['mon_s']) | (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "else:\n",
    "    idx = (ds.time.dt.month >= ar_dict['mon_s']) & (ds.time.dt.month <= ar_dict['mon_e'])\n",
    "\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# # Select AR days JUST IN R01\n",
    "# idx = (ds.ar >= 1) & (ds.location == 'R01')\n",
    "# Select AR days in all subregions\n",
    "idx = (ds.ar >= 1)\n",
    "ds_ar = ds.sel(time=idx)\n",
    "# ds_ar = ds\n",
    "\n",
    "# print results\n",
    "print(ds_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of independent AR events and their duration in days\n",
    "x = ds.ar\n",
    "event_id, total_events, duration = persistence(x)\n",
    "print('Total number of AR days in season: ', x.sum())\n",
    "print('Total number of independent AR events: ', total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load dataset into memory\n",
    "ds_ar = ds_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of variable arrays\n",
    "    \n",
    "var_list = []\n",
    "for i in range(len(config_dict['varlist'])):\n",
    "    var1 = config_dict['varlist'][i]\n",
    "    domain = config_dict['domain'][i]\n",
    "    domain_bnds = config2['eof_domain'][domain]\n",
    "    lonslice = slice(domain_bnds[0], domain_bnds[1])\n",
    "    latslice = slice(domain_bnds[3], domain_bnds[2])\n",
    "    if pathvar == 'huvq':\n",
    "        lev = config_dict['varlev'][i]\n",
    "        var_list.append(ds_ar[var1].sel(lon=lonslice, lat=latslice, level=lev))\n",
    "    if pathvar == 'ivt':\n",
    "        var_list.append(ds_ar[var1].sel(lon=lonslice, lat=latslice))\n",
    "\n",
    "# Check that sizes of arrays match\n",
    "for i, in_array in enumerate(var_list):\n",
    "    # Extract variable as numpy array\n",
    "    var1 = in_array.values\n",
    "    print(var1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "var_list = spatial_weights(var_list)\n",
    "\n",
    "## Flatten data to [time x space]\n",
    "var_list = flatten_array(var_list)\n",
    "\n",
    "## Center data\n",
    "var_list = center_data(var_list)\n",
    "\n",
    "## Standardize Arrays by removing the mean and dividing by the standard deviation of the columns\n",
    "## For multivariate, place into single flattened array\n",
    "Xs = standardize_arrays(var_list, mode=eofmode, dispersion_matrix=dispmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute eigenvalues & eigenvectors\n",
    "R, evals, evecs = calc_eigs(z=Xs, mode=eofmode)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')\n",
    "\n",
    "# save eigenvalues and eigenvectors\n",
    "outfile = out_path + 'eigenvalues_' + eofmode + dispmat + '.txt'\n",
    "np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "outfile = out_path + 'eigenvectors_' + eofmode + dispmat + '.txt'\n",
    "np.savetxt(outfile, evecs, fmt='%.5f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of EOFs that are separated based on the North Test\n",
    "n = []\n",
    "i = 0\n",
    "while lower[i] > upper[i+1]:\n",
    "    n.append(i+1)\n",
    "    i += 1\n",
    "print('Number of separated EOFs: ', max(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOFs and PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose neofs and npcs based on North Test - where separated\n",
    "# to save, plot, etc.\n",
    "neofs = max(n)\n",
    "npcs = neofs\n",
    "\n",
    "# Calculate EOFS (spatial modes)\n",
    "eofs = calc_eofs(Xs, evecs, evals, neofs, mode=eofmode)\n",
    "\n",
    "# Split eofs into separate arrays for each variable\n",
    "ntim, nlat, nlon = var1.shape\n",
    "npts = nlat*nlon\n",
    "nvar = len(var_list)\n",
    "# Reshape spatial dim back to 2D map\n",
    "eofmodes = var_list\n",
    "for i in np.arange(len(var_list)):\n",
    "    tmp = eofs[:,i*npts:(i+1)*npts]\n",
    "    eofmodes[i] = np.reshape(tmp, (neofs,nlat,nlon))\n",
    "    \n",
    "# Calculate PCs (time coefficients)\n",
    "pcs = calc_pcs(Xs, evecs, evals, npcs, mode=eofmode)\n",
    "# results in [ntim, npcs] to plot in PC plot\n",
    "\n",
    "## loadings*\n",
    "## in the case of t-mode these are our \"pcs\" or time-coefficients\n",
    "## in the case of s-mode, these are our \"eofs\" or spatial loadings\n",
    "loads = pcs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting \n",
    "lons = ds_ar.lon.data\n",
    "lats = ds_ar.lat.data\n",
    "udat = eofmodes[1][0:neofs,:,:]\n",
    "vdat = eofmodes[2][0:neofs,:,:]\n",
    "data = eofmodes[0][0:neofs,:,:]\n",
    "\n",
    "print(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "pc_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    pc_label.append(\"PC{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict of the first n eofs with column labels for df\n",
    "values = []\n",
    "for i in range(neofs):\n",
    "    values.append(loads[:,i])\n",
    "    \n",
    "keys = eof_label\n",
    "dicts = dict(zip(keys, values))\n",
    "# print(dicts)\n",
    "\n",
    "# Create new dataframe\n",
    "dates_arDays = ds_ar.time.values\n",
    "df_out = pd.DataFrame(dicts, index=dates_arDays)\n",
    "\n",
    "# Save loadings\n",
    "outfile = out_path + 'loadings_' + eofmode + dispmat + '.csv'\n",
    "# np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n",
    "df_out.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = fig_path + 'spatial_' + eofmode + dispmat + '.png'\n",
    "nrows = neofs\n",
    "ncols = 1\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='5%',\n",
    "                label_mode='')\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Contour Filled\n",
    "    clevs = np.arange(-30,31,5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    # Wind barbs / vectors\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=20) \n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('m', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# FUNCTIONS\n",
    "\n",
    "def plot_optimal_k(data, neofs, kmax, create_plot=False, filename=None):\n",
    "    \"\"\" Elbow plot to determine optimal number of clusters (k) \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        data to perform clustering on where rows= obs and cols = variables\n",
    "    kmax : scalar, int\n",
    "        maximum number of clusters; iteration over the interval k=[1,kmax]\n",
    "    filename : string (optional)\n",
    "        filename or path to save figure. Default is None (fig not saved).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cohesion = sum of sq dist of samples to their cluster center\n",
    "    kclusters = np.arange(1, kmax, dtype=int) + 1\n",
    "    cohesion = np.empty((neofs-1, kmax-1))\n",
    "    silhouette = np.empty((neofs-1, kmax-1))\n",
    "    upper = np.empty((neofs-1, kmax-1))\n",
    "    lower = np.empty((neofs-1, kmax-1))\n",
    "    \n",
    "    # iterate over EOFs\n",
    "    for i in range(neofs-1):\n",
    "        X = data[:,0:i+2]\n",
    "        \n",
    "        # iterate over kclusters\n",
    "        for j in range(len(kclusters)):\n",
    "            ki = kclusters[j]\n",
    "            km = KMeans(n_clusters=kclusters[j])\n",
    "            cluster_labels = km.fit_predict(X)\n",
    "            km = km.fit(X)\n",
    "            cohesion[i, j] = (km.inertia_)\n",
    "\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            silhouette[i, j] = silhouette_score(X, cluster_labels)\n",
    "            # Compute the silhouette scores for each sample\n",
    "            sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "            \n",
    "            spread = []\n",
    "            for m in range(ki):\n",
    "                # Aggregate the silhouette scores for samples belonging to\n",
    "                # cluster i, and sort them\n",
    "                ith_cluster_silhouette_values = \\\n",
    "                    sample_silhouette_values[cluster_labels == m]\n",
    "\n",
    "                ith_cluster_silhouette_values.sort()\n",
    "                spread.append(ith_cluster_silhouette_values.max())\n",
    "            \n",
    "            lower[i, j] = min(spread)\n",
    "            upper[i, j] = max(spread)\n",
    "\n",
    "            \n",
    "    if create_plot == True:\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches((6.0,10.0))\n",
    "        fig.dpi = 300\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        colors = ['tab:blue', 'tab:red', 'tab:green', 'k']\n",
    "        \n",
    "        # Elbow plot\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "        for k in range(neofs-1):\n",
    "            coh = cohesion[k, :]\n",
    "            ax.plot(kclusters, coh, c=colors[k], marker='o', linewidth=2.0, markersize=7.0)\n",
    "#             ax.set_title('Elbow Plot for Optimal K')\n",
    "            ax.set_ylabel('Sum of Sq Dist (cohesion)')\n",
    "            ax.set_xlabel('k (# of clusters)')\n",
    "            ax.set_xlim(1., kmax+1)\n",
    "            ax.set_ylim(0, 210)\n",
    "            ax.set_xticks(kclusters)\n",
    "        \n",
    "        # Silhouette plot\n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        for k in range(neofs-1):\n",
    "            sil = silhouette[k, :]\n",
    "            ax2.plot(kclusters, sil, c=colors[k], marker='o', linewidth=2.0, markersize=7.0)\n",
    "            ax2.fill_between(kclusters, lower[k, :], upper[k, :], fc=colors[k], ec=None, alpha=0.2)\n",
    "            ax2.set_ylabel('Mean Silhouette')\n",
    "            ax2.set_xlabel('k (# of clusters)')\n",
    "            ax2.set_xlim(1., kmax+1)\n",
    "            ax2.set_ylim(0, 1.)\n",
    "            ax2.set_xticks(kclusters)\n",
    "        \n",
    "        if filename:\n",
    "            # Save the figure\n",
    "            fig.savefig(filename, bbox_inches='tight', dpi=fig.dpi)\n",
    "            fig.clf()\n",
    "    else:\n",
    "        return kclusters, cohesion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal K\n",
    "# maximum number of clusters (number of iterations)\n",
    "kmax = 15\n",
    "\n",
    "# Elbow plot\n",
    "outfile = fig_path + 'elbow_' + eofmode + dispmat + '.png'\n",
    "plot_optimal_k(loads, neofs, kmax, create_plot=True, filename=outfile)\n",
    "\n",
    "print(outfile)\n",
    "display(Image(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine optimal k by examining the kde of the eofs\n",
    "g = sns.PairGrid(df_out, diag_sharey=True, corner=True)\n",
    "# g.map_upper(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw = 3)\n",
    "g = g.add_legend(fontsize=14)\n",
    "\n",
    "outfile = fig_path + 'hist_kde_' + eofmode + dispmat + '.png'\n",
    "g.savefig(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar_types",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
