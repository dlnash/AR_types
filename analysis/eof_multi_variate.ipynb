{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis of H, U and V at 250 hPa in Extratropics and Q, U, and V at 850 hPa in Tropics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import cycle\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mticker\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence\n",
    "from eofs import *\n",
    "from ar_funcs import preprocess_ar_area_subregions\n",
    "from kmeans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "path_to_data = '/home/nash/DATA/data/'                            # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/cEOF_tropics_extratropics/'      # figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select MERRA2 or ERA5\n",
    "reanalysis = 'era5'\n",
    "\n",
    "if reanalysis == 'era5':\n",
    "    start_date = '1979-01-01'\n",
    "    end_date = '2018-12-31'\n",
    "    filename = 'ar_catalog_ERAI_fraction_HASIAsubregions.nc'\n",
    "## if MERRA2\n",
    "else:\n",
    "    start_date = '1980-01-01'\n",
    "    end_date = '2017-12-31'\n",
    "    filename = 'ar_catalog_fraction_HASIAsubregions.nc'\n",
    "    \n",
    "f1 = path_to_data + 'CH1_generated_data/' + filename\n",
    "ds = xr.open_dataset(f1)\n",
    "# Set dates\n",
    "ds = ds.sel(time=slice(start_date, end_date))\n",
    "## Preprocess AR subregions - get dataframe of AR days based on area threshold\n",
    "df = preprocess_ar_area_subregions(df=ds.to_dataframe(), thres=0.3)\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set variable names (for saving data/figs)\n",
    "var_names = 'HUV250QUV850'\n",
    "eofmode = 't' # s or t\n",
    "dispmat = 'cor' # dispersion matrix type correlation/covariance\n",
    "\n",
    "# Select lat/lon grid \n",
    "# Tropics/Extratropics Domain\n",
    "lonmin = 0\n",
    "lonmax = 120\n",
    "latmin = -15\n",
    "latmax = 65\n",
    "\n",
    "lev = [850., 250.]\n",
    "\n",
    "# for figure names for testing different configurations\n",
    "fname_id = var_names + eofmode + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + dispmat\n",
    "\n",
    "def preprocess(ds):\n",
    "    '''keep only selected lats and lons'''\n",
    "    return ds.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), level=lev)\n",
    "\n",
    "# open HUV anomaly data\n",
    "filepath_pattern = path_to_data + 'ERA5/huvq/anomalies/daily_filtered_anomalies_huvq_*.nc'\n",
    "    \n",
    "    \n",
    "f2 = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset to rename lat lon\n",
    "ds = xr.Dataset({'H': (['time', 'level', 'lat', 'lon'], f2['z'].values),\n",
    "                 'U': (['time', 'level',  'lat', 'lon'], f2['u'].values),\n",
    "                 'V': (['time', 'level',  'lat', 'lon'], f2['v'].values),\n",
    "                 'QV': (['time', 'level',  'lat', 'lon'], f2['q'].values)},\n",
    "                      coords={'time': (['time'], f2.time.values),\n",
    "                              'level': (['level'], f2.level.values),\n",
    "                              'lat': (['lat'], f2.latitude.values),\n",
    "                              'lon': (['lon'], f2.longitude.values)})\n",
    "\n",
    "ds\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Set variable names (for saving data/figs)\n",
    "# var_names = 'HUVQ500'\n",
    "# eofmode = 't' # s or t\n",
    "# dispmat = 'cor' # dispersion matrix type correlation/covariance\n",
    "\n",
    "# ## Select lat/lon grid \n",
    "# # Tropics/Extratropics Domain\n",
    "# lonmin = 0\n",
    "# lonmax = 120\n",
    "# latmin = -15\n",
    "# latmax = 65\n",
    "\n",
    "# # for figure names for testing different configurations\n",
    "# fname_id = var_names + eofmode + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + dispmat\n",
    "\n",
    "# ### MERRA2 DATA ###\n",
    "# def preprocess(ds):\n",
    "#     '''keep only selected lats and lons'''\n",
    "#     return ds.sel(lat=slice(latmin, latmax), lon=slice(lonmin, lonmax))\n",
    "\n",
    "# # open H data\n",
    "# filepath_pattern = path_to_data + 'MERRA2/anomalies/H500/daily_*.nc'\n",
    "\n",
    "# ds_h = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, concat_dim='time', combine='by_coords')\n",
    "# print('ds size in GB {:0.2f}\\n'.format(ds_h.nbytes / 1e9))\n",
    "\n",
    "# # open QV data\n",
    "# filepath_pattern = path_to_data + 'MERRA2/anomalies/QV500/daily_*.nc'\n",
    "\n",
    "# ds_q = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, concat_dim='time', combine='by_coords')\n",
    "# print('ds size in GB {:0.2f}\\n'.format(ds_q.nbytes / 1e9))\n",
    "\n",
    "# ## open UV data\n",
    "# filepath_pattern = path_to_data + 'MERRA2/anomalies/UV500/daily_*.nc'\n",
    "# ds_uv = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "# print('ds size in GB {:0.2f}\\n'.format(ds_uv.nbytes / 1e9))\n",
    "\n",
    "# ## combine H and UV data into 1 ds object\n",
    "# ds = xr.merge([ds_h, ds_uv.U, ds_uv.V, ds_q])\n",
    "# # ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AR time series to merra; set as coordinate variables\n",
    "ds['ar'] = ('time', df.ar)\n",
    "ds = ds.set_coords('ar')\n",
    "\n",
    "ds['location'] = ('time', df.location)\n",
    "ds = ds.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim date range\n",
    "if reanalysis == 'era5':\n",
    "    start_date = '1979-12-01'\n",
    "    end_date = '2018-02-28'\n",
    "    ys = 1979\n",
    "    ye = 2018\n",
    "else:\n",
    "    start_date = '1980-12-01'\n",
    "    end_date = '2017-02-28'\n",
    "    ys = 1980\n",
    "    ye = 2017\n",
    "    \n",
    "idx = slice(start_date, end_date)\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# Select DJF months\n",
    "idx = (ds.time.dt.month >= 12) | (ds.time.dt.month <= 2)\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# # Select AR days JUST IN R01\n",
    "# idx = (ds.ar >= 1) & (ds.location == 'R01')\n",
    "# Select AR days in all subregions\n",
    "idx = (ds.ar >= 1)\n",
    "ds_ar = ds.sel(time=idx)\n",
    "# ds_ar = ds\n",
    "\n",
    "# print results\n",
    "print(ds_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of independent AR events\n",
    "\n",
    "years = np.arange(ys, ye) \n",
    "nyrs = len(years)\n",
    "total_events = 0\n",
    "for k in range(nyrs-1):    \n",
    "    # Extract single DJF season\n",
    "    date1 = \"{}-12-01\".format(years[k])\n",
    "    date2 = \"{}-02-28\".format(years[k+1])\n",
    "    x = ds.ar.sel(time=slice(date1,date2)).values\n",
    "    # Count AR events in that season\n",
    "    tags, tmp = persistence(x)\n",
    "    # Add to running event count\n",
    "    total_events += tmp\n",
    "\n",
    "print(\"Number of independent AR events: \", total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load merra_ar dataset into memory\n",
    "ds_ar = ds_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Tropics\" Domain\n",
    "tlonmin = 0\n",
    "tlonmax = 120\n",
    "tlatmin = -15\n",
    "tlatmax =  25\n",
    "\n",
    "# \"Extratropics\" Domain\n",
    "etlonmin = 0\n",
    "etlonmax = 120\n",
    "etlatmin = 25\n",
    "etlatmax =  65\n",
    "\n",
    "## Create list of variable arrays\n",
    "## FOR MERRA2 need to switch back to slice(latmin, latmax)\n",
    "# Extratropic variables\n",
    "var1 = ds_ar.U.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmax, etlatmin), level=250.)\n",
    "var2 = ds_ar.V.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmax, etlatmin), level=250.)\n",
    "var3 = ds_ar.H.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmax, etlatmin), level=250.)\n",
    "\n",
    "# Tropics variables\n",
    "var4 = ds_ar.U.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmax, tlatmin), level=850.)\n",
    "var5 = ds_ar.V.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmax, tlatmin), level=850.)\n",
    "var6 = ds_ar.QV.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmax, tlatmin), level=850.)\n",
    "\n",
    "var_list = [var1, var2, var3, var4, var5, var6]\n",
    "\n",
    "# Check that sizes of arrays match\n",
    "for i, in_array in enumerate(var_list):\n",
    "    # Extract variable as numpy array\n",
    "    var1 = in_array.values\n",
    "    print(var1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "var_list = spatial_weights(var_list)\n",
    "\n",
    "## Flatten data to [time x space]\n",
    "var_list = flatten_array(var_list)\n",
    "\n",
    "## Center data\n",
    "var_list = center_data(var_list)\n",
    "\n",
    "## Standardize Arrays by removing the mean and dividing by the standard deviation of the columns\n",
    "## For multivariate, place into single flattened array\n",
    "Xs = standardize_arrays(var_list, mode=eofmode, dispersion_matrix=dispmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute eigenvalues & eigenvectors\n",
    "R, evals, evecs = calc_eigs(z=Xs, mode=eofmode)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot dispersion matrix\n",
    "plt.contourf(R)\n",
    "plt.colorbar()\n",
    "title = 'Dispersion Matrix (' + dispmat + ')'\n",
    "plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2: Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot data\n",
    "xvals = np.arange(neofs) + 1\n",
    "ax.bar(xvals, pctvar[0:neofs], yerr=err[0:neofs], \n",
    "       color='tab:blue', alpha=0.8)\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel('EOF')\n",
    "ax.set_xticks(xvals)\n",
    "\n",
    "# y-axis\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "yticks = np.arange(0,16,3)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticks) \n",
    "\n",
    "# save fig\n",
    "filepath = path_to_figs + 'exp_variance_' + fname_id + '.png'\n",
    "plt.savefig(filepath, dpi=300)\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOFs and PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EOFS (spatial modes)\n",
    "neofs = 4\n",
    "eofs = calc_eofs(Xs, evecs, evals, neofs, mode=eofmode)\n",
    "\n",
    "# Split eofs into separate arrays for each variable\n",
    "ntim, nlat, nlon = var2.shape\n",
    "npts = nlat*nlon\n",
    "nvar = len(var_list)\n",
    "# Reshape spatial dim back to 2D map\n",
    "eofmodes = var_list\n",
    "for i in np.arange(len(var_list)):\n",
    "    tmp = eofs[:,i*npts:(i+1)*npts]\n",
    "    eofmodes[i] = np.reshape(tmp, (neofs,nlat,nlon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PCs (time coefficients)\n",
    "npcs = 4\n",
    "pcs = calc_pcs(Xs, evecs, evals, npcs, mode=eofmode)\n",
    "# results in [ntim, npcs] to plot in PC plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Eigenvectors, eigenvalues, and temporal loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in the case of t-mode these are our \"pcs\" or time-coefficients\n",
    "## in the case of s-mode, these are our \"eofs\" or spatial loadings\n",
    "loads = pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eigenvalues, eigenvectors, and loadings\n",
    "\n",
    "neofs = 4   # number of EOFs to save (evecs, loadings3)\n",
    "\n",
    "outfile = path_to_out + 'eigenvalues_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "outfile = path_to_out + 'eigenvectors_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, evecs[:,0:neofs], fmt='%.5f', delimiter=',')\n",
    "\n",
    "outfile = path_to_out + 'loadings_'+ fname_id + '.txt'\n",
    "np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3: Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Plot of Spatial Modes\n",
    "\n",
    "# Data for plotting extratropics\n",
    "lons_et = var2.lon.data\n",
    "lats_et = var2.lat.data\n",
    "udat_et = eofmodes[0][0:neofs,:,:]\n",
    "vdat_et = eofmodes[1][0:neofs,:,:]\n",
    "data_et = eofmodes[2][0:neofs,:,:]\n",
    "\n",
    "# Data for plotting tropics\n",
    "lons_t = var4.lon.data\n",
    "lats_t = var4.lat.data\n",
    "udat_t = eofmodes[3][0:neofs,:,:]\n",
    "vdat_t = eofmodes[4][0:neofs,:,:]\n",
    "data_t = eofmodes[5][0:neofs,:,:]\n",
    "\n",
    "print(np.min(data_et), np.max(data_et))\n",
    "print(np.min(data_t), np.max(data_t))\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "pc_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    pc_label.append(\"PC{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = path_to_figs + 'eofs_'+ fname_id + '.png'\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "# sns.set_style('ticks')\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='2.5%',\n",
    "                label_mode='')\n",
    "\n",
    "#newcmap = cmocean.tools.crop_by_percent(cmo.matter, 15, which='max', N=None)\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "#     ax = draw_basemap(ax, extent=None, xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Add contour fill plot for extratropics\n",
    "#     clevs = np.arange(-15,16,2)\n",
    "    clevs = np.arange(-50,55,2)\n",
    "    cf = ax.contourf(lons_et, lats_et, data_et[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "#     # add vectors for extratropics\n",
    "#     ax.quiver(lons_et, lats_et, udat_et[k,:,:], vdat_et[k,:,:], transform=datacrs,\n",
    "#               color='black', pivot='middle', regrid_shape=30) \n",
    "    \n",
    "    # Add contour fill plot for q500\n",
    "    cf = ax.contourf(lons_t, lats_t, data_t[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "#     # add vectors for tropics\n",
    "#     ax.quiver(lons_t, lats_t, udat_t[k,:,:], vdat_t[k,:,:], transform=datacrs,\n",
    "#               color='black', pivot='middle', regrid_shape=30)\n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('m', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,11))\n",
    "fig.dpi = 200\n",
    "fname = path_to_figs + 'pc_'+ fname_id\n",
    "fmt = 'png'\n",
    "results = loads[:,:4]\n",
    "\n",
    "X, nplots = results.shape\n",
    "x = np.arange(len(results))\n",
    "\n",
    "for i in np.arange(nplots):\n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    ax.plot(x, results[:,i], '-')\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.set_ylim(-1, 1)\n",
    "#     ax.set_ylabel('Normalized Units')\n",
    "    # subtitles\n",
    "    ax.set_title(pc_label[i], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[i], loc='right', fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.35, wspace=0.003)\n",
    "\n",
    "fig.savefig('%s.%s' %(fname, fmt), bbox_inches='tight', dpi=fig.dpi)\n",
    "fig.clf()\n",
    "\n",
    "\n",
    "plotFile = fname + '.png'\n",
    "print(plotFile)\n",
    "display(Image(plotFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF represented as correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put PCS into dataset object\n",
    "npcs = 2\n",
    "ds_pc = xr.Dataset({'pc': (['time', 'pcs'], loads[:,0:npcs])},\n",
    "                      coords={'time': (['time'], ds_ar.time.values),\n",
    "                              'pcs': (['pcs'], np.arange(npcs))})\n",
    "ds_pc.pc.sel(pcs=0)\n",
    "\n",
    "# select the pc to correlate\n",
    "pc1 = ds_pc.pc.sel(pcs=0)\n",
    "pc2 = ds_pc.pc.sel(pcs=1)\n",
    "# calculate statistics between AR ds and PCs\n",
    "cor1, pval1, tstat1 = correlation_pvalue(x=pc1, y=ds_ar, lagx=0, lagy=0, n=total_events)\n",
    "cor2, pval2, tstat2 = correlation_pvalue(x=pc2, y=ds_ar, lagx=0, lagy=0, n=total_events)\n",
    "\n",
    "# ## combine cor1 and cor2 into 1 ds object\n",
    "ds_cor = xr.concat([cor1, cor2], dim='pcs')\n",
    "ds_pval = xr.concat([pval1, pval2], dim='pcs')\n",
    "ds_tstat = xr.concat([tstat1, tstat2], dim='pcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Plot of Correlation with EOF1 and EOF2\n",
    "\n",
    "# Data for plotting extratropics\n",
    "lons = ds_ar.lon.data\n",
    "lats = ds_ar.lat.data\n",
    "\n",
    "# Set up projection\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "nplots=8\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "pool = cycle(['1', '2'])\n",
    "pool2 = cycle([0, 1])\n",
    "for k in range(nplots):\n",
    "    neof = next(pool)\n",
    "    eof_label.append(\"EOF{0}\".format(neof))\n",
    "    \n",
    "var_label = ['H', 'H', 'QV', 'QV', 'V', 'V', 'U', 'U']    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 250 hPa correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = path_to_figs + 'cors_huvq250'+ fname_id + '.png'\n",
    "nrows = 4\n",
    "ncols = 2\n",
    "# sns.set_style('ticks')\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='2.5%',\n",
    "                label_mode='')\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    # variable to plot\n",
    "    var_plot = var_label[k]\n",
    "    neof = next(pool2)\n",
    "#     data = ds_cor[var_plot].sel(level=250., pcs=neof).where(np.abs(ds_pval[var_plot].sel(level=250., pcs=neof))<=0.05)\n",
    "    data = ds_cor[var_plot].sel(level=250., pcs=neof).where(np.abs(ds_tstat[var_plot].sel(level=250., pcs=neof))>=1.96)\n",
    "#     data = ds_cor[var_plot].sel(level=250., pcs=neof)\n",
    "    # Add contour fill plot for correlation maps\n",
    "    clevs = np.arange(-1,1.1,0.1)\n",
    "    cf = ax.contourf(lons, lats, data, transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    \n",
    "#     # Add contours for pval\n",
    "#     kw_clabels = {'fontsize': 11, 'inline': True, 'inline_spacing': 5, 'fmt': '%0.2f',\n",
    "#                   'rightside_up': True, 'use_clabeltext': True}\n",
    "#     clevs = np.arange(0.05, 2.25, 0.25)\n",
    "#     pvaldata = ds_pval[var_plot].sel(level=250., pcs=neof)\n",
    "#     cs = ax.contour(lons, lats, pvaldata, levels=clevs, colors='k',\n",
    "#                         linewidths=1.0, linestyles='dashed', transform=datacrs)\n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k]+' 250 hPa', loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('correlation coefficient', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 850 hPa correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "filepath = path_to_figs + 'cors_huvq850'+ fname_id + '.png'\n",
    "nrows = 4\n",
    "ncols = 2\n",
    "# sns.set_style('ticks')\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, \n",
    "                111, \n",
    "                axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), \n",
    "                axes_pad = 0.55,\n",
    "                cbar_location='bottom', \n",
    "                cbar_mode='single',\n",
    "                cbar_pad=0.0, \n",
    "                cbar_size='2.5%',\n",
    "                label_mode='')\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "    var_plot = var_label[k]\n",
    "    neof = next(pool2)\n",
    "#     data = ds_cor[var_plot].sel(level=850., pcs=neof).where(np.abs(ds_pval[var_plot].sel(level=850., pcs=neof))<=0.05)\n",
    "    data = ds_cor[var_plot].sel(level=850., pcs=neof).where(np.abs(ds_tstat[var_plot].sel(level=850., pcs=neof))>=1.96)\n",
    "#     data = ds_cor[var_plot].sel(level=850., pcs=neof)\n",
    "    # Add contour fill plot for correlation maps\n",
    "    clevs = np.arange(-1,1.1,0.1)\n",
    "    cf = ax.contourf(lons, lats, data, transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k]+' 850 hPa', loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('correlation coefficient', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatterplot of PC1 v. PC2\n",
    "x = loads[:, 0]\n",
    "y = loads[:, 1]\n",
    "\n",
    "plt.plot(x, y, 'o', color='black')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal K\n",
    "\n",
    "# maximum number of clusters (number of iterations)\n",
    "kmax =15\n",
    "# number of eofs\n",
    "neofs = 2\n",
    "# input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Elbow plot\n",
    "outfile = path_to_figs + 'elbow'+ fname_id\n",
    "plot_optimal_k(xdata, kmax, filename=outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means cluster analysis\n",
    "\n",
    "# Number of clusters\n",
    "nk = 2\n",
    "\n",
    "# Input data\n",
    "xdata = loads[:,0:neofs]\n",
    "\n",
    "# Compute k means and assign each point to a cluster\n",
    "kmeans = KMeans(n_clusters=nk)\n",
    "kmeans.fit(xdata)\n",
    "cluster = kmeans.predict(xdata)\n",
    "\n",
    "# LLJ category labels (llj days only)\n",
    "ar_cat = cluster + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of days in each cluster\n",
    "klabels, counts = np.unique(ar_cat, return_counts=True)\n",
    "\n",
    "# Save counts to txt file\n",
    "res = np.column_stack((klabels,counts))\n",
    "headstr = 'AR_TYPE, COUNT'\n",
    "outfile = path_to_out + fname_id + 'k_counts.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%d', header=headstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids (nclust x neofs)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Save centroids to txt file\n",
    "res = np.column_stack((klabels,centroids))\n",
    "headstr = \"AR_TYPE, EOF1, EOF2, EOF3, EOF4\"\n",
    "outfile = path_to_out + fname_id + 'centroids.txt'\n",
    "np.savetxt(outfile, res, delimiter=',', fmt='%s', header=headstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save AR location, loadings (EOF1-2), and category label (AR days only)\n",
    "\n",
    "# Vector of AR dates\n",
    "dates_arDays =  ds_ar.time.values\n",
    "\n",
    "# Create new dataframe\n",
    "data = {'LOC':ds_ar.location.values,\n",
    "        'EOF1':loads[:,0],\n",
    "        'EOF2':loads[:,1],\n",
    "        'AR_CAT':ar_cat}\n",
    "df_out = pd.DataFrame(data, index=dates_arDays)\n",
    "\n",
    "## Create EOF \"category\" based on if exceeds std dev \n",
    "std_PC = np.std(df_out)\n",
    "mean_PC = np.mean(df_out)\n",
    "\n",
    "df_out['EOF_CAT1'] = 0\n",
    "df_out['EOF_CAT2'] = 0\n",
    "df_out['EOF_CAT'] = 0\n",
    "idx = (df_out['EOF1']-mean_PC.EOF1 >= std_PC.EOF1) | (df_out['EOF1']-mean_PC.EOF1 <= std_PC.EOF1*-1)\n",
    "df_out.loc[idx, 'EOF_CAT1'] = 1\n",
    "df_out.loc[idx, 'EOF_CAT'] = 1\n",
    "\n",
    "idx = (df_out['EOF2']-mean_PC.EOF2 >= std_PC.EOF2) | (df_out['EOF2']-mean_PC.EOF2 <= std_PC.EOF2*-1)\n",
    "df_out.loc[idx, 'EOF_CAT2'] = 1\n",
    "df_out.loc[idx, 'EOF_CAT'] = 2\n",
    "print(df_out)\n",
    "# Export dataframe as csv\n",
    "outfile = path_to_out + fname_id + 'hma_AR-types-loadings.csv'\n",
    "df_out.to_csv(outfile)\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save time series of all DJF days with AR types\n",
    "\n",
    "# Arrays with ALL DJF days\n",
    "dates_allDays = ds.time.values\n",
    "ar_cat_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "eof_cat_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "eof_cat1_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "eof_cat2_allDays = np.zeros(len(dates_allDays), dtype=int)\n",
    "\n",
    "# Loop over ar days and match to ar_full \n",
    "for i, date in enumerate(dates_arDays):\n",
    "    idx = np.where(dates_allDays == date)\n",
    "    ar_cat_allDays[idx] = ar_cat[i]\n",
    "    eof_cat_allDays[idx] = df_out.EOF_CAT.values[i]\n",
    "    eof_cat1_allDays[idx] = df_out.EOF_CAT1.values[i]\n",
    "    eof_cat2_allDays[idx] = df_out.EOF_CAT2.values[i]\n",
    "\n",
    "# Create dataframe\n",
    "data = {'AR_CAT':ar_cat_allDays,\n",
    "        'EOF_CAT':eof_cat_allDays,\n",
    "        'EOF_CAT1':eof_cat1_allDays,\n",
    "        'EOF_CAT2':eof_cat2_allDays}\n",
    "df_out = pd.DataFrame(data, index=dates_allDays)\n",
    "print(df_out)\n",
    "\n",
    "outfile = path_to_out + fname_id + 'hma_AR-types-djf.csv'\n",
    "df_out.to_csv(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ar_types)",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
