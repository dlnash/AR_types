{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF Analysis of AR days\n",
    "\n",
    "* Multivariate EOF analysis of H, U and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as  pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as mticker\n",
    "# cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "# plot styles/formatting\n",
    "import seaborn as sns\n",
    "import cmocean.cm as cmo\n",
    "import cmocean\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from plotter import draw_basemap\n",
    "from timeseries import persistence\n",
    "from eofs import *\n",
    "from ar_funcs import preprocess_ar_area_subregions\n",
    "from kmeans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "\n",
    "# home = Path.home()                                # users home directory\n",
    "# root = home/'DATA'/'repositories'/'AR_types'      # project root directory\n",
    "path_to_data = '/home/nash/DATA/data/'                 # project data -- read only\n",
    "path_to_out  = '/home/nash/DATA/repositories/AR_types/out/'                          # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '/home/nash/DATA/repositories/AR_types/figs/'                        # figures\n",
    "\n",
    "# # check that path exists\n",
    "# path_to_figs.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default font for all matplotlib text (can only set this ONCE; must restart kernel to change it)\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'   # set the default font family to 'sans-serif'\n",
    "rcParams['font.sans-serif'] = 'Arial'    # set the default sans-serif font to 'Arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R01</th>\n",
       "      <th>R02</th>\n",
       "      <th>R03</th>\n",
       "      <th>ar</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-01-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            R01  R02       R03  ar location\n",
       "time                                       \n",
       "1980-01-01  0.0  0.0  0.000000   0      NaN\n",
       "1980-01-02  0.0  0.0  0.072829   0      NaN\n",
       "1980-01-03  0.0  0.0  0.000000   0      NaN\n",
       "1980-01-04  0.0  0.0  0.000000   0      NaN\n",
       "1980-01-05  0.0  0.0  0.000000   0      NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read netCDF with fraction of area AR covers each subregion\n",
    "filename = path_to_data + 'CH1_generated_data/ar_catalog_fraction_HASIAsubregions.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "\n",
    "# Set dates\n",
    "ds = ds.sel(time=slice('1980-01-01', '2017-12-31'))\n",
    "\n",
    "## Preprocess AR subregions - get dataframe of AR days based on area threshold\n",
    "df = preprocess_ar_area_subregions(df=ds.to_dataframe(), thres=0.3)\n",
    "# Show table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERRA2 reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds size in GB 3.88\n",
      "\n",
      "ds size in GB 7.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Set variable names (for saving data/figs)\n",
    "var_names = 'HUV500'\n",
    "\n",
    "## Select lat/lon grid \n",
    "# # HASIA Domain\n",
    "# lonmin = 0\n",
    "# lonmax = 120\n",
    "# latmin = 0\n",
    "# latmax =  50\n",
    "\n",
    "# Tropics/Extratropics Domain\n",
    "lonmin = 0\n",
    "lonmax = 120\n",
    "latmin = -25\n",
    "latmax = 65\n",
    "\n",
    "\n",
    "### MERRA2 DATA ###\n",
    "def preprocess(ds):\n",
    "    '''keep only selected lats and lons'''\n",
    "    return ds.sel(lat=slice(latmin, latmax), lon=slice(lonmin, lonmax))\n",
    "\n",
    "# open H data\n",
    "filepath_pattern = path_to_data + 'MERRA2/anomalies/H500/daily_*.nc'\n",
    "\n",
    "ds_h = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, concat_dim='time', combine='by_coords')\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds_h.nbytes / 1e9))\n",
    "\n",
    "# # open QV data\n",
    "# filepath_pattern = path_to_data + 'MERRA2/anomalies/QV500/daily_*.nc'\n",
    "\n",
    "# ds_q = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, concat_dim='time', combine='by_coords')\n",
    "# print('ds size in GB {:0.2f}\\n'.format(ds_q.nbytes / 1e9))\n",
    "\n",
    "\n",
    "## open UV data\n",
    "filepath_pattern = path_to_data + 'MERRA2/anomalies/UV500/daily_*.nc'\n",
    "ds_uv = xr.open_mfdataset(filepath_pattern, preprocess=preprocess, combine='by_coords')\n",
    "print('ds size in GB {:0.2f}\\n'.format(ds_uv.nbytes / 1e9))\n",
    "\n",
    "## combine H and UV data into 1 ds object\n",
    "merra = xr.merge([ds_h, ds_uv.U, ds_uv.V])\n",
    "# merra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 181, lon: 193, time: 13880)\n",
      "Coordinates:\n",
      "    lev        float64 500.0\n",
      "  * lon        (lon) float64 0.0 0.625 1.25 1.875 ... 118.1 118.8 119.4 120.0\n",
      "  * lat        (lat) float64 -25.0 -24.5 -24.0 -23.5 ... 63.5 64.0 64.5 65.0\n",
      "  * time       (time) datetime64[ns] 1980-01-01T09:00:00 ... 2017-12-31T09:00:00\n",
      "    dayofyear  (time) int64 dask.array<chunksize=(366,), meta=np.ndarray>\n",
      "    ar         (time) int64 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "    location   (time) object nan nan nan nan nan nan ... nan nan nan nan nan nan\n",
      "Data variables:\n",
      "    H          (time, lat, lon) float64 dask.array<chunksize=(366, 181, 193), meta=np.ndarray>\n",
      "    U          (time, lat, lon) float64 dask.array<chunksize=(366, 181, 193), meta=np.ndarray>\n",
      "    V          (time, lat, lon) float64 dask.array<chunksize=(366, 181, 193), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Add AR time series to merra; set as coordinate variables\n",
    "merra['ar'] = ('time', df.ar)\n",
    "merra = merra.set_coords('ar')\n",
    "\n",
    "merra['location'] = ('time', df.location)\n",
    "merra = merra.set_coords('location')\n",
    "\n",
    "# print dataset\n",
    "print(merra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 181, lon: 193, time: 213)\n",
      "Coordinates:\n",
      "    lev        float64 500.0\n",
      "  * lon        (lon) float64 0.0 0.625 1.25 1.875 ... 118.1 118.8 119.4 120.0\n",
      "  * lat        (lat) float64 -25.0 -24.5 -24.0 -23.5 ... 63.5 64.0 64.5 65.0\n",
      "  * time       (time) datetime64[ns] 1980-12-12T09:00:00 ... 2017-02-18T09:00:00\n",
      "    dayofyear  (time) int64 dask.array<chunksize=(2,), meta=np.ndarray>\n",
      "    ar         (time) int64 1 1 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "    location   (time) object 'R01' 'R01' 'R01' 'R01' ... 'R01' 'R01' 'R01' 'R01'\n",
      "Data variables:\n",
      "    H          (time, lat, lon) float64 dask.array<chunksize=(2, 181, 193), meta=np.ndarray>\n",
      "    U          (time, lat, lon) float64 dask.array<chunksize=(2, 181, 193), meta=np.ndarray>\n",
      "    V          (time, lat, lon) float64 dask.array<chunksize=(2, 181, 193), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Trim date range\n",
    "start_date = '1980-12-01'\n",
    "end_date = '2017-02-28'\n",
    "idx = slice(start_date, end_date)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# Select DJF months\n",
    "idx = (merra.time.dt.month >= 12) | (merra.time.dt.month <= 2)\n",
    "merra = merra.sel(time=idx)\n",
    "\n",
    "# Select AR days JUST IN R01\n",
    "idx = (merra.ar >= 1) & (merra.location == 'R01')\n",
    "merra_ar = merra.sel(time=idx)\n",
    "\n",
    "# print results\n",
    "print(merra_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of independent AR events:  215\n"
     ]
    }
   ],
   "source": [
    "# Count number of independent AR events\n",
    "\n",
    "years = np.arange(1980, 2018) \n",
    "nyrs = len(years)\n",
    "total_events = 0\n",
    "for k in range(nyrs-1):    \n",
    "    # Extract single DJF season\n",
    "    date1 = \"{}-12-01\".format(years[k])\n",
    "    date2 = \"{}-02-28\".format(years[k+1])\n",
    "    x = merra.ar.sel(time=slice(date1,date2)).values\n",
    "    # Count AR events in that season\n",
    "    tags, tmp = persistence(x)\n",
    "    # Add to running event count\n",
    "    total_events += tmp\n",
    "\n",
    "print(\"Number of independent AR events: \", total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, center, and standardize data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.67 s, sys: 3min 47s, total: 3min 53s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load merra_ar dataset into memory\n",
    "merra_ar = merra_ar.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(arr_list):\n",
    "    ''' Remove the mean of an array along the first dimension.\n",
    "    \n",
    "    If *True*, the mean along the first axis of *dataset* (the\n",
    "    time-mean) will be removed prior to analysis. If *False*,\n",
    "    the mean along the first axis will not be removed. Defaults\n",
    "    to *True* (mean is removed).\n",
    "    The covariance interpretation relies on the input data being\n",
    "    anomaly data with a time-mean of 0. Therefore this option\n",
    "    should usually be set to *True*. Setting this option to\n",
    "    *True* has the useful side effect of propagating missing\n",
    "    values along the time dimension, ensuring that a solution\n",
    "    can be found even if missing values occur in different\n",
    "    locations at different times.\n",
    "    '''\n",
    "    for i, in_array in enumerate(arr_list):\n",
    "        # Compute the mean along the first dimension.\n",
    "        mean = in_array.mean(axis=0, skipna=False)\n",
    "        # Return the input array with its mean along the first dimension\n",
    "        # removed.\n",
    "        arr_list[i] = in_array - mean\n",
    "    return arr_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Tropics\" Domain\n",
    "tlonmin = 0\n",
    "tlonmax = 120\n",
    "tlatmin = -25\n",
    "tlatmax =  25\n",
    "\n",
    "# \"Extratropics\" Domain\n",
    "etlonmin = 0\n",
    "etlonmax = 120\n",
    "etlatmin = 25\n",
    "etlatmax =  65\n",
    "\n",
    "## Create list of variable arrays\n",
    "# Extratropic variables\n",
    "var1 = merra_ar.U.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmin, etlatmax))\n",
    "var2 = merra_ar.V.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmin, etlatmax))\n",
    "var3 = merra_ar.H.sel(lon=slice(etlonmin, etlonmax), lat=slice(etlatmin, etlatmax))\n",
    "\n",
    "# Tropics variables\n",
    "var4 = merra_ar.U.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmin, tlatmax))\n",
    "var5 = merra_ar.V.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmin, tlatmax))\n",
    "# var6 = merra_ar.QV.sel(lon=slice(tlonmin, tlonmax), lat=slice(tlatmin, tlatmax))\n",
    "\n",
    "var_list = [var1, var2, var3, var4, var5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the variables by removing long-term mean\n",
    "var_list = center_data(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_weights(arr_list):\n",
    "    \"\"\"Spatial weights\n",
    "    \n",
    "    Returns a 1D array of weights equal to the sqrt of the cos of latitude.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr_list : list\n",
    "        list of variable arrays\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    weighted_arrays : list of arrays\n",
    "        weights equal to the sqrt of the cosine of latitude\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    # Apply spatial weights using xarray\n",
    "    wgts = spatial_weights(lats)            # compute weights\n",
    "    era['wgts'] = ('latitude', wgts)        # add `wgts` to dataset\n",
    "    era['uwnd_wgt'] = era.uwnd * era.wgts   # apply wgts to data variable\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, in_array in enumerate(arr_list):\n",
    "        latitude = in_array.lat\n",
    "        # convert lats from degrees to radians\n",
    "        lat_rad = np.deg2rad(latitude)\n",
    "        # compute weights\n",
    "        weights = np.sqrt(np.cos(lat_rad))\n",
    "        # apply spatial weights to array\n",
    "        arr_list[i] = in_array*weights\n",
    "    return arr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 162 ms, sys: 175 ms, total: 337 ms\n",
      "Wall time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "var_list = spatial_weights(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(X):\n",
    "    # Find the indices of values that are missing in the whole row\n",
    "    nonMissingIndex = np.where(np.logical_not(np.isnan(X[0])))[0]\n",
    "    # Remove missing values from the design matrix.\n",
    "    dataNoMissing = X[:, nonMissingIndex]\n",
    "    return nonMissingIndex, dataNoMissing\n",
    "\n",
    "def valid_nan(in_array):\n",
    "    inan = np.isnan(in_array)\n",
    "    return (inan.any(axis=0) == inan.all(axis=0)).all()\n",
    "\n",
    "def standardize_and_flatten_arrays(arr_list, mode='t'):\n",
    "    \n",
    "    \n",
    "    for i, in_array in enumerate(arr_list):\n",
    "        # Extract variable as numpy array\n",
    "            var1 = in_array.values\n",
    "\n",
    "        # Data dimensions\n",
    "            ntim, nlat, nlon = var1.shape\n",
    "            npts = nlat*nlon\n",
    "\n",
    "        # Reshape into 2D arrays by flattening the spatial dimension\n",
    "            tmp1 = np.reshape(var1, (ntim, npts))\n",
    "\n",
    "        # Remove missing data\n",
    "            tmp1_idx, tmp1_miss = remove_missing_values(tmp1)\n",
    "\n",
    "        ## Test if the removal of nans was successful\n",
    "            print(valid_nan(tmp1_miss))\n",
    "        \n",
    "        # Data dimensions with missing values removed\n",
    "            ntim, npts = tmp1_miss.shape\n",
    "        \n",
    "        # if t-mode\n",
    "            if mode == 't':\n",
    "                X1 = tmp1_miss.T\n",
    "        # if s-mode\n",
    "            else:\n",
    "                X1 = tmp1_miss\n",
    "\n",
    "        # Standardize by columns\n",
    "            x1std = np.std(X1, axis=0)\n",
    "            X1s = X1 / x1std\n",
    "            \n",
    "            arr_list[i] = X1s\n",
    "\n",
    "    # Combine variables into single data matrix Xs\n",
    "    nvar = len(arr_list)\n",
    "    # if t-mode\n",
    "    if mode == 't':\n",
    "        Xs = np.empty((nvar*npts,ntim))\n",
    "        Xs[0:npts,:] = X1s\n",
    "        Xs[npts:npts*2,:]  = X2s\n",
    "        Xs[npts*2:,:]  = X3s\n",
    "    \n",
    "    # if s-mode\n",
    "    else:\n",
    "        Xs = np.empty((ntim, nvar*npts))\n",
    "        Xs[:, 0:npts] = X1s\n",
    "        Xs[:, npts:npts*2]  = X2s\n",
    "        Xs[:, npts*2:]  = X3s\n",
    "    \n",
    "    print(Xs.shape)\n",
    "\n",
    "    # Check that column means=0 and std dev=1\n",
    "    test = np.mean(np.mean(Xs, axis=0))\n",
    "    print(\"Column means: \", np.round(test,2))\n",
    "    test = np.mean(np.std(Xs, axis=0))\n",
    "    print(\"Column std: \", np.round(test,2))\n",
    "    \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n",
      "(213, 46899)\n",
      "Column means:  0.0\n",
      "Column std:  1.0\n",
      "(213, 46899)\n",
      "Column means:  0.0\n",
      "Column std:  1.0\n",
      "CPU times: user 1.27 s, sys: 381 ms, total: 1.65 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Center the variables by removing long-term mean\n",
    "var1 = center_data(merra_ar.U)\n",
    "var2 = center_data(merra_ar.V)\n",
    "var3 = center_data(merra_ar.H)\n",
    "\n",
    "# Weight the data by the square root of the cosine of the lat\n",
    "wgts = spatial_weights(merra_ar.lat)\n",
    "var1 = var1*wgts\n",
    "var2 = var2*wgts\n",
    "var3 = var3*wgts\n",
    "\n",
    "# Extract variables as numpy arrays\n",
    "var1 = var1.values\n",
    "var2 = var2.values\n",
    "var3 = var3.values\n",
    "\n",
    "# Data dimensions\n",
    "ntim, nlat, nlon = var1.shape\n",
    "npts = nlat*nlon\n",
    "nvar = 2\n",
    "\n",
    "# Reshape into 2D arrays by flattening the spatial dimension\n",
    "tmp1 = np.reshape(var1, (ntim, npts))\n",
    "tmp2 = np.reshape(var2, (ntim, npts))\n",
    "tmp3 = np.reshape(var3, (ntim, npts))\n",
    "\n",
    "# Remove missing data\n",
    "tmp1_idx, tmp1_miss = remove_missing_values(tmp1)\n",
    "tmp2_idx, tmp2_miss = remove_missing_values(tmp2)\n",
    "tmp3_idx, tmp3_miss = remove_missing_values(tmp3)\n",
    "\n",
    "## Test if the removal of nans was successful\n",
    "print(valid_nan(tmp1_miss), valid_nan(tmp2_miss), valid_nan(tmp3_miss))\n",
    "\n",
    "## Standardize and flatten variable arrays\n",
    "Xs = standardize_and_flatten_arrays(tmp1_miss, tmp2_miss, tmp3_miss, 3)\n",
    "Xs_nomiss = standardize_and_flatten_arrays(tmp1, tmp2, tmp3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:  (46899,)\n",
      "[2.33323782e-310+4.63730928e-310j 4.63730928e-310+4.63730928e-310j\n",
      " 2.09648107e-001+3.17796061e-001j ... 4.81070986e-001+5.74286252e-001j\n",
      " 6.66205700e-001+7.58229402e-001j 8.43238066e-001+9.21142115e-001j] \n",
      "\n",
      "Eigenvectors:  (46899, 46899)\n",
      "[[0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
      " ...\n",
      " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]] \n",
      "\n",
      "CPU times: user 5min 22s, sys: 3min 54s, total: 9min 16s\n",
      "Wall time: 9min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute eigenvalues & eigenvectors\n",
    "evals, evecs = calc_eofs(Xs)\n",
    "\n",
    "print('Eigenvalues: ', evals.shape)\n",
    "print(evals, '\\n')\n",
    "\n",
    "print('Eigenvectors: ', evecs.shape)\n",
    "print(np.round(evecs, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_variance(eig):\n",
    "    var_eig = (eig/sum(eig))*100.\n",
    "    return var_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative variance explained by the first 0 EOFs:\n",
      "100.02+0.00j% \n",
      "\n",
      "Cumulative variance explained by the first 4 EOFs:\n",
      "-0.01-0.00j% \n",
      "\n",
      "1 \t -0.00-0.00j%\n",
      "2 \t -0.00-0.00j%\n",
      "3 \t -0.01-0.00j%\n",
      "4 \t -0.01-0.00j%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percent explained var by each eigenvector\n",
    "pctvar = pct_variance(evals)\n",
    "\n",
    "# Number of EOFs that explain more than 1% of the total variance\n",
    "idx = pctvar[pctvar >= 1.0]\n",
    "neofs = len(idx)\n",
    "\n",
    "# print exp var >= 1.0\n",
    "cumvar = np.sum(pctvar[0:neofs-1])\n",
    "print(f'Cumulative variance explained by the first {neofs} EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var: neofs = 4\n",
    "cumvar = np.sum(pctvar[0:3])\n",
    "print(f'Cumulative variance explained by the first 4 EOFs:')\n",
    "print(f'{cumvar:.2f}% \\n')\n",
    "\n",
    "# print exp var for 4 eofs\n",
    "for k in range(4):\n",
    "    print(f'{k+1} \\t {pctvar[k]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = north_test(evals, total_events)\n",
    "upper = pctvar + err\n",
    "lower = pctvar - err\n",
    "\n",
    "print(np.round(upper[0:6],3))\n",
    "print(np.round(pctvar[0:6],3))\n",
    "print(np.round(lower[0:6],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2: Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\", {'patch.force_edgecolor':False})\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot data\n",
    "xvals = np.arange(neofs) + 1\n",
    "ax.bar(xvals, pctvar[0:neofs], yerr=err[0:neofs], \n",
    "       color='tab:blue', alpha=0.8)\n",
    "\n",
    "# x-axis\n",
    "ax.set_xlabel('EOF')\n",
    "ax.set_xticks(xvals)\n",
    "\n",
    "# y-axis\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "yticks = np.arange(0,16,3)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticks) \n",
    "\n",
    "# save fig\n",
    "filepath = path_to_figs + 'exp_variance_' + var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + '.png'\n",
    "plt.savefig(filepath, dpi=300)\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neofs = 19\n",
    "loads = loadings(evals, evecs, neofs)\n",
    "\n",
    "print(loads.shape)\n",
    "print(np.round(loads,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eigenvalues, eigenvectors, and loadings\n",
    "\n",
    "neofs = 4   # number of EOFs to save (evecs, loadings3)\n",
    "\n",
    "outfile = path_to_out + 'eigenvalues_'+ var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + '.txt'\n",
    "np.savetxt(outfile, evals, fmt='%.5f')\n",
    "\n",
    "outfile = path_to_out + 'eigenvectors_'+ var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + '.txt'\n",
    "np.savetxt(outfile, evecs[:,0:neofs], fmt='%.5f', delimiter=',')\n",
    "\n",
    "outfile = path_to_out + 'loadings_'+ var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + '.txt'\n",
    "np.savetxt(outfile, loads[:,0:neofs], fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate principal components (spatial modes)\n",
    "neofs = 19\n",
    "pcs = calc_pcs(Xs_nomiss, evecs, neofs)\n",
    "# pcs = calc_pcs(Xs, evecs, neofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pcs into separate arrays for each variable\n",
    "tmp1 = pcs[:,0:npts]\n",
    "tmp2 = pcs[:,npts:npts*2]\n",
    "tmp3 = pcs[:,npts*2:]\n",
    "# tmp1 = pcs_comb[:,0:nlat*nlon]\n",
    "# tmp2 = pcs_comb[:,nlat*nlon:]\n",
    "\n",
    "# Reshape spatial dim back to 2D map\n",
    "pcmodes_var1 = np.reshape(tmp1, (neofs,nlat,nlon))\n",
    "pcmodes_var2 = np.reshape(tmp2, (neofs,nlat,nlon))\n",
    "pcmodes_var3 = np.reshape(tmp3, (neofs,nlat,nlon))\n",
    "#print(pcmodes_var1.shape, pcmodes_var2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3: Spatial Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Plot of Spatial Modes\n",
    "\n",
    "# number of eofs to plot\n",
    "neofs = 4\n",
    "\n",
    "# Data for plotting\n",
    "lons = merra_ar.lon.data\n",
    "lats = merra_ar.lat.data\n",
    "udat = pcmodes_var1[0:neofs,:,:]\n",
    "vdat = pcmodes_var2[0:neofs,:,:]\n",
    "data = pcmodes_var3[0:neofs,:,:]\n",
    "\n",
    "print(np.nanmin(data), np.nanmax(data))\n",
    "\n",
    "# Set up projection\n",
    "# mapcrs = ccrs.PlateCarree()\n",
    "mapcrs = ccrs.NorthPolarStereo(central_longitude=60.0)\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# Set tick/grid locations\n",
    "dx = np.arange(lonmin,lonmax+20,20)\n",
    "dy = np.arange(latmin,latmax+20,20)\n",
    "\n",
    "# subtitles\n",
    "eof_label = [ ]\n",
    "var_label = [ ]\n",
    "for k in range(neofs):\n",
    "    eof_label.append(\"EOF{:1d}\".format(k+1,))\n",
    "    var_label.append(\"{:.2f}%\".format(pctvar[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4.0 ,4.0))\n",
    "fig.dpi = 300\n",
    "fname = path_to_figs + 'eofs_'+ var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + 'NPS'\n",
    "fmt = 'png'\n",
    "\n",
    "for k in np.arange(len(data)):\n",
    "    ax = plt.subplot(2, 2, k+1, projection=mapcrs)\n",
    "#     ax.set_extent([lons.min(), lons.max(), lats.min(), 90.], crs=mapcrs)\n",
    "    # Add contour fill plot\n",
    "    clevs = np.arange(-25,27.5,2.5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    # add vectors\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=30)      \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='0.4', linewidth=0.3)\n",
    "#     ax.add_feature(cfeature.BORDERS, edgecolor='0.4', linewidth=0.3)\n",
    "\n",
    "    ## Add in meridian and parallels\n",
    "    gl = ax.gridlines(linewidth=.25, color='black', alpha=0.7, linestyle='--')\n",
    "\n",
    "#     gl.xlocator = mticker.FixedLocator(np.arange(-180., 200., 20))\n",
    "#     gl.ylocator = mticker.FixedLocator(np.arange(20., 70., 10.))\n",
    "#     gl.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl.yformatter = LATITUDE_FORMATTER\n",
    "    \n",
    "# # add colorbar [left, bottom, width, height]\n",
    "ax2 = fig.add_axes([0.13, 0.05, 0.77, 0.02])\n",
    "cbar = fig.colorbar(cf, cax=ax2, drawedges=True, \n",
    "                    orientation='horizontal', extendfrac='auto')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.set_label('m', fontsize=10)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "fig.savefig('%s.%s' %(fname, fmt), bbox_inches='tight', dpi=fig.dpi)\n",
    "fig.clf()\n",
    "\n",
    "\n",
    "plotFile = fname + '.png'\n",
    "print(plotFile)\n",
    "# display(Image(plotFile))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = plt.figure(figsize=(10,11))\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "mapcrs = ccrs.PlateCarree()\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Set up Axes Grid\n",
    "axes_class = (GeoAxes,dict(map_projection=mapcrs))\n",
    "axgr = AxesGrid(fig, 111, axes_class=axes_class,\n",
    "                nrows_ncols=(nrows, ncols), axes_pad = 0.55,\n",
    "                cbar_location='bottom', cbar_mode='single',\n",
    "                cbar_pad=0.0, cbar_size='2.5%',label_mode='')\n",
    "\n",
    "#newcmap = cmocean.tools.crop_by_percent(cmo.matter, 15, which='max', N=None)\n",
    "\n",
    "# Loop for drawing each plot\n",
    "for k, ax in enumerate(axgr):\n",
    "    ax = draw_basemap(ax, extent=[lonmin,lonmax,latmin,latmax], xticks=dx, yticks=dy)\n",
    "#     ax = draw_basemap(ax, extent=None, xticks=dx, yticks=dy)\n",
    "    \n",
    "    # Add contour fill plot\n",
    "    clevs = np.arange(-25,27.5,2.5)\n",
    "    cf = ax.contourf(lons, lats, data[k,:,:], transform=datacrs,\n",
    "                     levels=clevs,\n",
    "                     cmap=\"bwr\", extend='both')\n",
    "    # add vectors\n",
    "    ax.quiver(lons, lats, udat[k,:,:], vdat[k,:,:], transform=datacrs,\n",
    "              color='black', pivot='middle', regrid_shape=20)      \n",
    "    # subtitles\n",
    "    ax.set_title(eof_label[k], loc='left', fontsize=12)\n",
    "    ax.set_title(var_label[k], loc='right', fontsize=12)\n",
    "    \n",
    "# single colorbar\n",
    "cb = fig.colorbar(cf, axgr.cbar_axes[0], orientation='horizontal', drawedges=True)\n",
    "cb.set_label('m', fontsize=11)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "    \n",
    "# Display figure\n",
    "filepath = path_to_figs + 'eofs_'+ var_names + str(lonmin) + str(lonmax) + str(latmin) + str(latmax) + '.png'\n",
    "plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ar_types)",
   "language": "python",
   "name": "ar_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
